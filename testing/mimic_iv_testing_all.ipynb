{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0DsFARgE0MOh"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QwocLnbR-p9h"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "4jqZLo8J1VcH",
    "outputId": "0e02ae17-665e-499b-d17c-b1f32542a4a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject</th>\n",
       "      <th>time(hr)</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>...</th>\n",
       "      <th>x3_8.0</th>\n",
       "      <th>x3_9.0</th>\n",
       "      <th>x3_nan</th>\n",
       "      <th>x4_Confused</th>\n",
       "      <th>x4_Inappropriate Words</th>\n",
       "      <th>x4_Incomprehensible sounds</th>\n",
       "      <th>x4_No Response</th>\n",
       "      <th>x4_No Response-ETT</th>\n",
       "      <th>x4_Oriented</th>\n",
       "      <th>x4_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.027222</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.060556</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.143889</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            subject  time(hr)  Diastolic blood pressure  \\\n",
       "0           0  11432534_episode1  0.000000                       NaN   \n",
       "1           1  11432534_episode1  0.010556                       NaN   \n",
       "2           2  11432534_episode1  0.027222                      42.0   \n",
       "3           3  11432534_episode1  0.060556                      42.0   \n",
       "4           4  11432534_episode1  0.143889                      42.0   \n",
       "\n",
       "   Fraction inspired oxygen  Glucose  Heart Rate  Height  Mean blood pressure  \\\n",
       "0                       NaN      NaN         NaN     NaN                  NaN   \n",
       "1                       NaN      NaN        84.0     NaN                  NaN   \n",
       "2                       NaN      NaN        84.0     NaN                 53.0   \n",
       "3                       NaN      NaN        86.0     NaN                 53.0   \n",
       "4                       NaN      NaN        86.0     NaN                 53.0   \n",
       "\n",
       "   Oxygen saturation  ...  x3_8.0  x3_9.0  x3_nan  x4_Confused  \\\n",
       "0                NaN  ...     0.0     0.0     1.0          0.0   \n",
       "1               93.0  ...     0.0     0.0     1.0          0.0   \n",
       "2               93.0  ...     0.0     0.0     1.0          0.0   \n",
       "3               92.0  ...     0.0     0.0     1.0          0.0   \n",
       "4               92.0  ...     0.0     0.0     0.0          1.0   \n",
       "\n",
       "   x4_Inappropriate Words  x4_Incomprehensible sounds  x4_No Response  \\\n",
       "0                     0.0                         0.0             0.0   \n",
       "1                     0.0                         0.0             0.0   \n",
       "2                     0.0                         0.0             0.0   \n",
       "3                     0.0                         0.0             0.0   \n",
       "4                     0.0                         0.0             0.0   \n",
       "\n",
       "   x4_No Response-ETT  x4_Oriented  x4_nan  \n",
       "0                 0.0          0.0     1.0  \n",
       "1                 0.0          0.0     1.0  \n",
       "2                 0.0          0.0     1.0  \n",
       "3                 0.0          0.0     1.0  \n",
       "4                 0.0          0.0     0.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Time Series Data\n",
    "\n",
    "# Data by the hour\n",
    "first_48_data = pd.read_csv('../../../../data/datasets/mimiciv_timeseries/mimiciv_timeseries.csv')\n",
    "\n",
    "\n",
    "first_48_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "6CcBAvNK-p9j",
    "outputId": "43a34369-9b8d-4d4f-a451-4184e18048c6"
   },
   "outputs": [],
   "source": [
    "for idx, row in first_48_data.iterrows():\n",
    "        \n",
    "    # Making gcs scores nan where unobserved\n",
    "    if row['x0_nan'] == 1:\n",
    "        first_48_data.at[idx, 'x0_0.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x0_1.0'] = np.nan\n",
    "\n",
    "    if row['x1_nan'] == 1:\n",
    "        first_48_data.at[idx, 'x1_None'] = np.nan\n",
    "        first_48_data.at[idx, 'x1_Spontaneously'] = np.nan\n",
    "        first_48_data.at[idx, 'x1_To Pain'] = np.nan\n",
    "        first_48_data.at[idx, 'x1_To Speech'] = np.nan\n",
    "\n",
    "    if row['x2_nan'] == 1:\n",
    "        first_48_data.at[idx, 'x2_Abnormal Flexion'] = np.nan\n",
    "        first_48_data.at[idx, 'x2_Abnormal extension'] = np.nan\n",
    "        first_48_data.at[idx, 'x2_Flex-withdraws'] = np.nan\n",
    "        first_48_data.at[idx, 'x2_Localizes Pain'] = np.nan\n",
    "        first_48_data.at[idx, 'x2_No response'] = np.nan\n",
    "        first_48_data.at[idx, 'x2_Obeys Commands'] = np.nan\n",
    "\n",
    "    if row['x3_nan'] == 1:\n",
    "        first_48_data.at[idx, 'x3_10.0'] = np.nan \n",
    "        first_48_data.at[idx, 'x3_11.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_12.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_13.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_14.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_15.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_3.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_4.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_5.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_6.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_7.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_8.0'] = np.nan\n",
    "        first_48_data.at[idx, 'x3_9.0'] = np.nan\n",
    "\n",
    "\n",
    "    if row['x4_nan'] == 1:\n",
    "        first_48_data.at[idx, 'x4_Confused'] = np.nan\n",
    "        first_48_data.at[idx, 'x4_Inappropriate Words'] = np.nan\n",
    "        first_48_data.at[idx, 'x4_Incomprehensible sounds'] = np.nan\n",
    "        first_48_data.at[idx, 'x4_No Response'] = np.nan\n",
    "        first_48_data.at[idx, 'x4_No Response-ETT'] = np.nan\n",
    "        first_48_data.at[idx, 'x4_Oriented'] = np.nan\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "RUR3NEqu-p9j",
    "outputId": "17c42f7b-4987-4b37-93a5-e0c8fd999379"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject</th>\n",
       "      <th>time(hr)</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>...</th>\n",
       "      <th>x3_8.0</th>\n",
       "      <th>x3_9.0</th>\n",
       "      <th>x3_nan</th>\n",
       "      <th>x4_Confused</th>\n",
       "      <th>x4_Inappropriate Words</th>\n",
       "      <th>x4_Incomprehensible sounds</th>\n",
       "      <th>x4_No Response</th>\n",
       "      <th>x4_No Response-ETT</th>\n",
       "      <th>x4_Oriented</th>\n",
       "      <th>x4_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.027222</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.060556</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.143889</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            subject  time(hr)  Diastolic blood pressure  \\\n",
       "0           0  11432534_episode1  0.000000                       NaN   \n",
       "1           1  11432534_episode1  0.010556                       NaN   \n",
       "2           2  11432534_episode1  0.027222                      42.0   \n",
       "3           3  11432534_episode1  0.060556                      42.0   \n",
       "4           4  11432534_episode1  0.143889                      42.0   \n",
       "\n",
       "   Fraction inspired oxygen  Glucose  Heart Rate  Height  Mean blood pressure  \\\n",
       "0                       NaN      NaN         NaN     NaN                  NaN   \n",
       "1                       NaN      NaN        84.0     NaN                  NaN   \n",
       "2                       NaN      NaN        84.0     NaN                 53.0   \n",
       "3                       NaN      NaN        86.0     NaN                 53.0   \n",
       "4                       NaN      NaN        86.0     NaN                 53.0   \n",
       "\n",
       "   Oxygen saturation  ...  x3_8.0  x3_9.0  x3_nan  x4_Confused  \\\n",
       "0                NaN  ...     NaN     NaN     1.0          NaN   \n",
       "1               93.0  ...     NaN     NaN     1.0          NaN   \n",
       "2               93.0  ...     NaN     NaN     1.0          NaN   \n",
       "3               92.0  ...     NaN     NaN     1.0          NaN   \n",
       "4               92.0  ...     0.0     0.0     0.0          1.0   \n",
       "\n",
       "   x4_Inappropriate Words  x4_Incomprehensible sounds  x4_No Response  \\\n",
       "0                     NaN                         NaN             NaN   \n",
       "1                     NaN                         NaN             NaN   \n",
       "2                     NaN                         NaN             NaN   \n",
       "3                     NaN                         NaN             NaN   \n",
       "4                     0.0                         0.0             0.0   \n",
       "\n",
       "   x4_No Response-ETT  x4_Oriented  x4_nan  \n",
       "0                 NaN          NaN     1.0  \n",
       "1                 NaN          NaN     1.0  \n",
       "2                 NaN          NaN     1.0  \n",
       "3                 NaN          NaN     1.0  \n",
       "4                 0.0          0.0     0.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_48_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "Y9z5-fNz-p9k",
    "outputId": "ca05b17e-590a-4237-81fa-d27a9636dc50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>episode_num</th>\n",
       "      <th>readmission</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18664949_episode1</td>\n",
       "      <td>18664949</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.114583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19793183_episode1</td>\n",
       "      <td>19793183</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.467361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15687156_episode1</td>\n",
       "      <td>15687156</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14504982_episode1</td>\n",
       "      <td>14504982</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.854167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            subject  subject_id  episode_num  readmission  \\\n",
       "0           0  11432534_episode1    11432534            1          0.0   \n",
       "1           1  18664949_episode1    18664949            1          0.0   \n",
       "2           2  19793183_episode1    19793183            1          0.0   \n",
       "3           3  15687156_episode1    15687156            1          0.0   \n",
       "4           4  14504982_episode1    14504982            1          0.0   \n",
       "\n",
       "   length_of_stay  mortality  \n",
       "0        7.935417          0  \n",
       "1        5.114583          0  \n",
       "2       20.467361          0  \n",
       "3        0.098611          1  \n",
       "4        9.854167          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading label data\n",
    "\n",
    "label_data = pd.read_csv('mimic_iv_label_data.csv')\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DoqGQrGl-p9k",
    "outputId": "f178a90f-20f0-4f0c-8c76-25bd34b73881"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10000980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10001217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10001725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10002013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  subject_id  cluster\n",
       "0           0    10000032        1\n",
       "1           1    10000980        1\n",
       "2           2    10001217        1\n",
       "3           3    10001725        1\n",
       "4           4    10002013        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading patient clusters\n",
    "\n",
    "patient_clusters = pd.read_csv('mimic_iv_patient_clusters.csv')\n",
    "patient_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8yeXbSiF-p9k"
   },
   "outputs": [],
   "source": [
    "# adding column for subject_id and episode_num\n",
    "\n",
    "subject_w_ep = first_48_data['subject']\n",
    "\n",
    "subject_ids = subject_w_ep.apply(lambda x: int(x.split('_')[0]))\n",
    "episode_nums = subject_w_ep.apply(lambda x: int(x.split('_')[1][7:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z_wZmqDX-p9l",
    "outputId": "be1ba51d-0e05-411c-c8fc-4fc0b37bc16d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject</th>\n",
       "      <th>time(hr)</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>...</th>\n",
       "      <th>x3_nan</th>\n",
       "      <th>x4_Confused</th>\n",
       "      <th>x4_Inappropriate Words</th>\n",
       "      <th>x4_Incomprehensible sounds</th>\n",
       "      <th>x4_No Response</th>\n",
       "      <th>x4_No Response-ETT</th>\n",
       "      <th>x4_Oriented</th>\n",
       "      <th>x4_nan</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>episode_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.027222</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.060556</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.143889</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            subject  time(hr)  Diastolic blood pressure  \\\n",
       "0           0  11432534_episode1  0.000000                       NaN   \n",
       "1           1  11432534_episode1  0.010556                       NaN   \n",
       "2           2  11432534_episode1  0.027222                      42.0   \n",
       "3           3  11432534_episode1  0.060556                      42.0   \n",
       "4           4  11432534_episode1  0.143889                      42.0   \n",
       "\n",
       "   Fraction inspired oxygen  Glucose  Heart Rate  Height  Mean blood pressure  \\\n",
       "0                       NaN      NaN         NaN     NaN                  NaN   \n",
       "1                       NaN      NaN        84.0     NaN                  NaN   \n",
       "2                       NaN      NaN        84.0     NaN                 53.0   \n",
       "3                       NaN      NaN        86.0     NaN                 53.0   \n",
       "4                       NaN      NaN        86.0     NaN                 53.0   \n",
       "\n",
       "   Oxygen saturation  ...  x3_nan  x4_Confused  x4_Inappropriate Words  \\\n",
       "0                NaN  ...     1.0          NaN                     NaN   \n",
       "1               93.0  ...     1.0          NaN                     NaN   \n",
       "2               93.0  ...     1.0          NaN                     NaN   \n",
       "3               92.0  ...     1.0          NaN                     NaN   \n",
       "4               92.0  ...     0.0          1.0                     0.0   \n",
       "\n",
       "   x4_Incomprehensible sounds  x4_No Response  x4_No Response-ETT  \\\n",
       "0                         NaN             NaN                 NaN   \n",
       "1                         NaN             NaN                 NaN   \n",
       "2                         NaN             NaN                 NaN   \n",
       "3                         NaN             NaN                 NaN   \n",
       "4                         0.0             0.0                 0.0   \n",
       "\n",
       "   x4_Oriented  x4_nan  subject_id  episode_num  \n",
       "0          NaN     1.0    11432534            1  \n",
       "1          NaN     1.0    11432534            1  \n",
       "2          NaN     1.0    11432534            1  \n",
       "3          NaN     1.0    11432534            1  \n",
       "4          0.0     0.0    11432534            1  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_48_data['subject_id'] = subject_ids\n",
    "first_48_data['episode_num'] = episode_nums\n",
    "\n",
    "first_48_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pET4PBlf-p9l"
   },
   "outputs": [],
   "source": [
    "first_48_data.rename(columns={\"time(hr)\": \"Hours\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IBM9yMkm-p9m",
    "outputId": "7efb9d5a-6d1d-42c3-8927-a1a3280bfd94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>subject</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>...</th>\n",
       "      <th>x4_Oriented</th>\n",
       "      <th>x4_nan</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>episode_num</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>readmission</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>mortality</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>3710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>3710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.027222</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>3710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.060556</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>3710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.143889</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>3710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x            subject     Hours  Diastolic blood pressure  \\\n",
       "0             0  11432534_episode1  0.000000                       NaN   \n",
       "1             1  11432534_episode1  0.010556                       NaN   \n",
       "2             2  11432534_episode1  0.027222                      42.0   \n",
       "3             3  11432534_episode1  0.060556                      42.0   \n",
       "4             4  11432534_episode1  0.143889                      42.0   \n",
       "\n",
       "   Fraction inspired oxygen  Glucose  Heart Rate  Height  Mean blood pressure  \\\n",
       "0                       NaN      NaN         NaN     NaN                  NaN   \n",
       "1                       NaN      NaN        84.0     NaN                  NaN   \n",
       "2                       NaN      NaN        84.0     NaN                 53.0   \n",
       "3                       NaN      NaN        86.0     NaN                 53.0   \n",
       "4                       NaN      NaN        86.0     NaN                 53.0   \n",
       "\n",
       "   Oxygen saturation  ...  x4_Oriented  x4_nan  subject_id  episode_num  \\\n",
       "0                NaN  ...          NaN     1.0    11432534            1   \n",
       "1               93.0  ...          NaN     1.0    11432534            1   \n",
       "2               93.0  ...          NaN     1.0    11432534            1   \n",
       "3               92.0  ...          NaN     1.0    11432534            1   \n",
       "4               92.0  ...          0.0     0.0    11432534            1   \n",
       "\n",
       "   Unnamed: 0_y  readmission  length_of_stay  mortality  Unnamed: 0  cluster  \n",
       "0             0          0.0        7.935417          0        3710        1  \n",
       "1             0          0.0        7.935417          0        3710        1  \n",
       "2             0          0.0        7.935417          0        3710        1  \n",
       "3             0          0.0        7.935417          0        3710        1  \n",
       "4             0          0.0        7.935417          0        3710        1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging data with labels and cluster to get correct sample\n",
    "\n",
    "first_48_data = first_48_data.merge(label_data, on=['subject', 'subject_id', 'episode_num'])\n",
    "first_48_data = first_48_data.merge(patient_clusters, on='subject_id')\n",
    "\n",
    "first_48_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAaiM7yR-p9m",
    "outputId": "2bfdda6f-9b9b-4c42-d5ee-40a778ea90f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31489\n"
     ]
    }
   ],
   "source": [
    "print(len(first_48_data.groupby('subject')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BxnOdPXS-p9m",
    "outputId": "e25b2ea9-16d1-49c3-cb6c-81a24179b438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0_x', 'subject', 'Hours', 'Diastolic blood pressure',\n",
      "       'Fraction inspired oxygen', 'Glucose', 'Heart Rate', 'Height',\n",
      "       'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate',\n",
      "       'Systolic blood pressure', 'Temperature', 'Weight', 'pH', 'x0_0.0',\n",
      "       'x0_1.0', 'x0_nan', 'x1_None', 'x1_Spontaneously', 'x1_To Pain',\n",
      "       'x1_To Speech', 'x1_nan', 'x2_Abnormal Flexion',\n",
      "       'x2_Abnormal extension', 'x2_Flex-withdraws', 'x2_Localizes Pain',\n",
      "       'x2_No response', 'x2_Obeys Commands', 'x2_nan', 'x3_10.0', 'x3_11.0',\n",
      "       'x3_12.0', 'x3_13.0', 'x3_14.0', 'x3_15.0', 'x3_3.0', 'x3_4.0',\n",
      "       'x3_5.0', 'x3_6.0', 'x3_7.0', 'x3_8.0', 'x3_9.0', 'x3_nan',\n",
      "       'x4_Confused', 'x4_Inappropriate Words', 'x4_Incomprehensible sounds',\n",
      "       'x4_No Response', 'x4_No Response-ETT', 'x4_Oriented', 'x4_nan',\n",
      "       'subject_id', 'episode_num', 'Unnamed: 0_y', 'readmission',\n",
      "       'length_of_stay', 'mortality', 'Unnamed: 0', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(first_48_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U7mmPqCb-p9m"
   },
   "outputs": [],
   "source": [
    "# keeping 'Unnamed: 0_x' and renaming to 'original_idx' to retain original indexes\n",
    "first_48_data = first_48_data.drop(columns=['Unnamed: 0_y', 'Unnamed: 0'])\n",
    "\n",
    "first_48_data = first_48_data.rename(columns={'Unnamed: 0_x': 'original_idx'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Tkox0UVY-p9n",
    "outputId": "ab0ef36b-a6fe-4a09-8a63-7842c82c3637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['original_idx', 'subject', 'Hours', 'Diastolic blood pressure',\n",
      "       'Fraction inspired oxygen', 'Glucose', 'Heart Rate', 'Height',\n",
      "       'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate',\n",
      "       'Systolic blood pressure', 'Temperature', 'Weight', 'pH', 'x0_0.0',\n",
      "       'x0_1.0', 'x0_nan', 'x1_None', 'x1_Spontaneously', 'x1_To Pain',\n",
      "       'x1_To Speech', 'x1_nan', 'x2_Abnormal Flexion',\n",
      "       'x2_Abnormal extension', 'x2_Flex-withdraws', 'x2_Localizes Pain',\n",
      "       'x2_No response', 'x2_Obeys Commands', 'x2_nan', 'x3_10.0', 'x3_11.0',\n",
      "       'x3_12.0', 'x3_13.0', 'x3_14.0', 'x3_15.0', 'x3_3.0', 'x3_4.0',\n",
      "       'x3_5.0', 'x3_6.0', 'x3_7.0', 'x3_8.0', 'x3_9.0', 'x3_nan',\n",
      "       'x4_Confused', 'x4_Inappropriate Words', 'x4_Incomprehensible sounds',\n",
      "       'x4_No Response', 'x4_No Response-ETT', 'x4_Oriented', 'x4_nan',\n",
      "       'subject_id', 'episode_num', 'readmission', 'length_of_stay',\n",
      "       'mortality', 'cluster'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(first_48_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LVALxCqV-p9n",
    "outputId": "07268b00-e08a-475a-b85c-380e741235a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Diastolic blood pressure', 'Fraction inspired oxygen', 'Glucose',\n",
      "       'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation',\n",
      "       'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight',\n",
      "       'pH', 'x0_0.0', 'x0_1.0', 'x0_nan', 'x1_None', 'x1_Spontaneously',\n",
      "       'x1_To Pain', 'x1_To Speech', 'x1_nan', 'x2_Abnormal Flexion',\n",
      "       'x2_Abnormal extension', 'x2_Flex-withdraws', 'x2_Localizes Pain',\n",
      "       'x2_No response', 'x2_Obeys Commands', 'x2_nan', 'x3_10.0', 'x3_11.0',\n",
      "       'x3_12.0', 'x3_13.0', 'x3_14.0', 'x3_15.0', 'x3_3.0', 'x3_4.0',\n",
      "       'x3_5.0', 'x3_6.0', 'x3_7.0', 'x3_8.0', 'x3_9.0', 'x3_nan',\n",
      "       'x4_Confused', 'x4_Inappropriate Words', 'x4_Incomprehensible sounds',\n",
      "       'x4_No Response', 'x4_No Response-ETT', 'x4_Oriented', 'x4_nan'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(first_48_data.columns[3:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "026_Ox-Y-p9n",
    "outputId": "a950d6f5-d19d-4f3e-df4c-9b33c1d78d2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_idx</th>\n",
       "      <th>subject</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>...</th>\n",
       "      <th>x4_No Response</th>\n",
       "      <th>x4_No Response-ETT</th>\n",
       "      <th>x4_Oriented</th>\n",
       "      <th>x4_nan</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>episode_num</th>\n",
       "      <th>readmission</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>mortality</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.027222</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.060556</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11432534_episode1</td>\n",
       "      <td>0.143889</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11432534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.935417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_idx            subject     Hours  Diastolic blood pressure  \\\n",
       "0             0  11432534_episode1  0.000000                       NaN   \n",
       "1             1  11432534_episode1  0.010556                       NaN   \n",
       "2             2  11432534_episode1  0.027222                      42.0   \n",
       "3             3  11432534_episode1  0.060556                      42.0   \n",
       "4             4  11432534_episode1  0.143889                      42.0   \n",
       "\n",
       "   Fraction inspired oxygen  Glucose  Heart Rate  Height  Mean blood pressure  \\\n",
       "0                       NaN      NaN         NaN     NaN                  NaN   \n",
       "1                       NaN      NaN        84.0     NaN                  NaN   \n",
       "2                       NaN      NaN        84.0     NaN                 53.0   \n",
       "3                       NaN      NaN        86.0     NaN                 53.0   \n",
       "4                       NaN      NaN        86.0     NaN                 53.0   \n",
       "\n",
       "   Oxygen saturation  ...  x4_No Response  x4_No Response-ETT  x4_Oriented  \\\n",
       "0                NaN  ...             NaN                 NaN          NaN   \n",
       "1               93.0  ...             NaN                 NaN          NaN   \n",
       "2               93.0  ...             NaN                 NaN          NaN   \n",
       "3               92.0  ...             NaN                 NaN          NaN   \n",
       "4               92.0  ...             0.0                 0.0          0.0   \n",
       "\n",
       "   x4_nan  subject_id  episode_num  readmission  length_of_stay  mortality  \\\n",
       "0     1.0    11432534            1          0.0        7.935417          0   \n",
       "1     1.0    11432534            1          0.0        7.935417          0   \n",
       "2     1.0    11432534            1          0.0        7.935417          0   \n",
       "3     1.0    11432534            1          0.0        7.935417          0   \n",
       "4     0.0    11432534            1          0.0        7.935417          0   \n",
       "\n",
       "   cluster  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_48_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vx7GR50X1onC",
    "outputId": "9e9b88e1-d8d5-44a9-b635-237fbfc0bfd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31489\n"
     ]
    }
   ],
   "source": [
    "# Grouping by admission\n",
    "\n",
    "data = first_48_data.groupby('subject')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "cuve4-08-p9o",
    "outputId": "33038025-fc06-4ef0-c00e-c8e36e1dd2b8"
   },
   "outputs": [],
   "source": [
    "i = 0  \n",
    "\n",
    "subjects = []\n",
    "subject_idx = []\n",
    "readm_label = []\n",
    "mortality_label = []\n",
    "los_label = []\n",
    "cluster = []\n",
    "\n",
    "\n",
    "for group_idx, group_rows in data:  \n",
    "    \n",
    "    subjects.append(group_idx)\n",
    "    subject_idx.append(i)\n",
    "    \n",
    "    readm_label.append(group_rows['readmission'].values[0])\n",
    "    mortality_label.append(group_rows['mortality'].values[0])\n",
    "    los_label.append(group_rows['length_of_stay'].values[0])\n",
    "    cluster.append(group_rows['cluster'].values[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # stores totals for variables\n",
    "    cur_matrix = np.empty([48, 48])\n",
    "    cur_matrix[:] = np.nan\n",
    "\n",
    "    # stores counts for variables\n",
    "    cur_counts = np.empty([48, 48])\n",
    "    cur_counts[:] = np.nan\n",
    "\n",
    "    cur_columns = group_rows.columns.values.tolist()\n",
    "    feature_columns = cur_columns[3:-6]\n",
    "\n",
    "    j = 0\n",
    "    for idx, row in group_rows.iterrows():\n",
    "        \n",
    "            \n",
    "        # Modifying cur_data to have data by the hour for 48 hours\n",
    "        if row['Hours'] < j+1 and j < 48:\n",
    "            for k in range(len(feature_columns)):\n",
    "                if not (np.isnan(group_rows.loc[idx, feature_columns[k]])):\n",
    "                    if np.isnan(cur_matrix[j, k]):\n",
    "                        cur_matrix[j, k] = group_rows.loc[idx, feature_columns[k]]\n",
    "                        cur_counts[j, k] = 1\n",
    "                    else:\n",
    "                        cur_matrix[j, k] += group_rows.loc[idx, feature_columns[k]]\n",
    "                        cur_counts[j, k] += 1\n",
    "                        \n",
    "        else:\n",
    "            if j >= 48:\n",
    "                break\n",
    "            else:\n",
    "                j += 1\n",
    "                \n",
    "\n",
    "    # Getting time series data\n",
    "\n",
    "    X_element = np.divide(cur_matrix, cur_counts)\n",
    "\n",
    "    if i == 0:\n",
    "\n",
    "        # Holds all of the multivariate time series\n",
    "        X = np.array([X_element])\n",
    "\n",
    "    else:\n",
    "        X = np.concatenate((X, np.array([X_element])))\n",
    "    \n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7LiqNzaB5TU",
    "outputId": "afa7e61a-f0c1-463f-ec44-48167f6907eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31489, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mi4GN6L19-81",
    "outputId": "23b23ed0-7626-4cb4-ee7d-ea3ab49c6389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n",
      "Overall missingness:  0.7139147301438598 \n",
      "\n",
      "Missingness in more observed samples:  0.11668875635078418 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting more observed sequences\n",
    "\n",
    "idxs = []\n",
    "\n",
    "props = []\n",
    "\n",
    "props_in_sample = []\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    \n",
    "    \n",
    "    cur_matrix = X[i,:,:]\n",
    "\n",
    "    flattened_matrix = cur_matrix.flatten()\n",
    "    \n",
    "    flattened_series = pd.Series(flattened_matrix)\n",
    "\n",
    "    prop_unobserved = len(flattened_series.loc[np.isnan(flattened_series)]) / len(flattened_series) \n",
    "    props.append(prop_unobserved)\n",
    "\n",
    "    if prop_unobserved < 0.2:\n",
    "        idxs.append(i)\n",
    "        props_in_sample.append(prop_unobserved)\n",
    "        \n",
    "print(len(idxs))\n",
    "print(\"Overall missingness: \", np.mean(props), \"\\n\")\n",
    "print(\"Missingness in more observed samples: \", np.mean(props_in_sample), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "onN_GBPa9_Eg"
   },
   "outputs": [],
   "source": [
    "X = X[idxs,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Bo1JhSRX-p9p",
    "outputId": "fd930d9c-a04e-49da-819a-3e5d0ec9587c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31489, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_idx</th>\n",
       "      <th>readmission</th>\n",
       "      <th>mortality</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032_episode1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000980_episode1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.806944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001217_episode1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.794444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001217_episode2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.914583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001725_episode1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.994444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subject  subject_idx  readmission  mortality  length_of_stay  \\\n",
       "0  10000032_episode1            0          0.0          0        2.222222   \n",
       "1  10000980_episode1            1          0.0          0        5.806944   \n",
       "2  10001217_episode1            2          1.0          0        6.794444   \n",
       "3  10001217_episode2            3          NaN          0        5.914583   \n",
       "4  10001725_episode1            4          0.0          0        2.994444   \n",
       "\n",
       "   cluster  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame({'subject':subjects, 'subject_idx':subject_idx, 'readmission':readm_label, 'mortality':mortality_label,\n",
    "                  'length_of_stay':los_label, 'cluster':cluster})\n",
    "subjects = []\n",
    "subject_idx = []\n",
    "readm_label = []\n",
    "mortality_label = []\n",
    "los_label = []\n",
    "cluster = []\n",
    "\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VdMpwHvc-DNU",
    "outputId": "750f47c0-d7ed-4c3c-ce1d-c38fca61b6ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_idx</th>\n",
       "      <th>readmission</th>\n",
       "      <th>mortality</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>10019777_episode1</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.767361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>10038081_episode1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.138889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>10048001_episode3</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6.665972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>10054277_episode1</td>\n",
       "      <td>158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.947222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>10080961_episode1</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.791667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subject  subject_idx  readmission  mortality  length_of_stay  \\\n",
       "72   10019777_episode1           72          0.0          0       16.767361   \n",
       "120  10038081_episode1          120          0.0          1       14.138889   \n",
       "142  10048001_episode3          142          NaN          0        6.665972   \n",
       "158  10054277_episode1          158          0.0          0        8.947222   \n",
       "219  10080961_episode1          219          0.0          0       17.791667   \n",
       "\n",
       "     cluster  \n",
       "72         1  \n",
       "120        1  \n",
       "142        1  \n",
       "158        1  \n",
       "219        1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.loc[idxs]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "T2z9KMmI1tVF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 33\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "X_train_minmax, X_test_minmax, y_train_minmax, y_test_minmax = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TsOvGDg-p9p",
    "outputId": "3ffc5bb7-8883-410b-8b2a-e78b649cc44b"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardizing the data\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X_train.shape[2]):\n",
    "    scalers[i] = preprocessing.StandardScaler()\n",
    "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i]) \n",
    "\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i]) \n",
    "\n",
    "for i in range(X.shape[2]):\n",
    "    X[:, :, i] = scalers[i].transform(X[:, :, i]) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardizing the data\n",
    "\n",
    "train_minmax_scalers = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(X_train_minmax.shape[2]):\n",
    "    train_minmax_scalers[i] = preprocessing.MinMaxScaler()\n",
    "    X_train_minmax[:, :, i] = train_minmax_scalers[i].fit_transform(X_train_minmax[:, :, i]) \n",
    "\n",
    "for i in range(X_test_minmax.shape[2]):\n",
    "    X_test_minmax[:, :, i] = train_minmax_scalers[i].transform(X_test_minmax[:, :, i]) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLDWuUYu-p9q",
    "outputId": "aeea64a5-f50c-47a8-a7ed-e2640b8629bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 48, 48)\n",
      "[0.1008474384851535, -0.009839816180565222, 0.4465087020166753, -0.04028108531610427, 0.004436633246742992, -0.0006324717186075764, -0.010035731632010797, 0.049367177388261796, -0.009839816180565222, 0.047535036119595364, -0.03163723747048672, 0.013278458457099998, -0.005349788890249156, -0.001149356985517815, 0.039871366646355426, 0.027730812807314247, 0.06578873852242385, 0.09566301468650718, 0.06843730963621754, 0.007257287473653423, -0.004056038981423898, -0.009839816180565222, 0.06792886930665248, 0.08890024355195154, -0.01705183627436635, -0.03945357011761232, -0.03450202471860697, -0.02722783221832657, -0.029921088132459594, -0.031397896178470636, -0.04576201607821401, -0.04960253126663431, -0.05036111450310299, -0.06082962477373569, -0.0666224676444671, -0.06848101637117618, -0.0915765117550585, -0.08678810157694287, -0.04829619954588055, -0.03327371635926144, -0.04376088974067566, -0.07425859428453403, -0.08588975960240035, -0.009839816180565222, -0.009839816180565222, -0.009839816180565222, -0.009839816180565222, 0.02264404235662959]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "all_feature_means = []\n",
    "\n",
    "# Reshaping to 2-dimensional data for imputation\n",
    "X_2d = np.reshape(X, (X.shape[0]*X.shape[1], X.shape[2]))\n",
    "\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer.fit(X_2d)\n",
    "X_2d = mean_imputer.transform(X_2d)\n",
    "\n",
    "\n",
    "for i in range(X_2d.shape[1]):\n",
    "    all_feature_means.append(np.mean(X_2d[:][i]))\n",
    "\n",
    "\n",
    "\n",
    "print(all_feature_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwJDX3Ai2GnQ",
    "outputId": "76c46414-5d11-498c-d2c2-f92ee06efd2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 48, 48)\n",
      "[-0.04370821554881273, -0.06326523988469764, -0.09074172839210155, -0.05179448315229875, -0.04457860973674408, -0.05741266590642132, -0.04889665502028307, -0.123854021366981, -1.2213807418900122e-16, -0.04014206218843929, -0.014116727938208768, 0.03769024123277858, -1.2213807418900122e-16, -0.005132757689000536, 0.15095737231317433, 0.04825915070104827, 0.0523222618584886, 0.03390838945090374, 0.04008344938357662, -0.06992275231488444, 0.021883354992353176, -0.08186312349999082, -0.0710193876710042, -0.10058135234096094, -0.038625734209505706, -0.03791259647809228, -0.03343326657825388, -0.0038383679569045925, 0.06974424648213351, 0.0012886800955950646, 0.01951048414084754, -0.052753589013548506, -0.01529230703537108, -0.04762089691474342, -0.03417563664587722, 0.03045340251984773, -0.0858715473344326, 0.0022296611528718757, -0.04699212964732524, -0.005276585765169715, -0.019623609397297414, 0.017155906563545088, 0.05403531993496241, -0.014439329245338728, 0.11691133852044722, -0.0231733974966483, 0.024636387574085785, -0.07899745833285443]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "train_feature_means = []\n",
    "\n",
    "# Reshaping to 2-dimensional data for imputation\n",
    "X_train_2d = np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "\n",
    "train_mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "train_mean_imputer.fit(X_train_2d)\n",
    "X_train_2d = train_mean_imputer.transform(X_train_2d)\n",
    "\n",
    "\n",
    "for i in range(X_train_2d.shape[1]):\n",
    "    train_feature_means.append(np.mean(X_train_2d[:][i]))\n",
    "\n",
    "\n",
    "\n",
    "print(train_feature_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q12dIA8z2hsx",
    "outputId": "bb61e147-7358-4d61-a050-ebeef2f4c98f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 48, 48)\n",
      "[0.17978906575944878, -0.0469914166333379, -0.038579958270870185, -0.01768487862479687, -0.07352666894477142, -0.006890047072223543, 0.03152075593228532, -0.04357566504338392, -0.06881092649496805, -0.0350763067398119, -0.07444412368695201, -0.06755312068350479, 0.005217816650605389, -0.09710098325535244, -0.055634325184662915, 0.026061446374716926, 0.01163300674302121, -0.046719327861834276, 0.003926091996108448, -0.06872199019280416, -0.016151735377329057, -0.02070519661168997, -0.04044234970777171, -0.01683127410438134, -0.03582980449624557, -0.06895404169781451, -0.10796897913552111, -0.07427039889809484, -0.02838221780865156, 0.03001721120524246, 0.007071473799392898, -0.013362059743543106, 0.08762758424851663, 0.03307716606943529, -0.01572330525992439, 0.11687409620882437, 0.06886804111854723, 0.021333577464583402, 0.06362921240131901, -0.0032624013689174967, 0.03683237031348819, -0.07601168501944383, -0.058795212830304706, -0.0051600849637185265, -0.02858315402095436, 0.03233000288248116, -0.033141123494931306, -0.05426889266627494]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "test_feature_means = []\n",
    "\n",
    "# Reshaping to 2-dimensional data for imputation\n",
    "X_test_2d = np.reshape(X_test, (X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "\n",
    "test_mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "test_mean_imputer.fit(X_test_2d)\n",
    "X_test_2d = test_mean_imputer.transform(X_test_2d)\n",
    "\n",
    "\n",
    "for i in range(X_test_2d.shape[1]):\n",
    "    test_feature_means.append(np.mean(X_test_2d[:][i]))\n",
    "\n",
    "\n",
    "\n",
    "print(test_feature_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 48, 48)\n",
      "[0.286944895064253, 0.24783517976920588, 0.22632800019453317, 0.2522047842620026, 0.23745737064221428, 0.23810278706018292, 0.24116956698761238, 0.22975369414699598, 0.2269161128669485, 0.24063531183428735, 0.2404823683486719, 0.24681804330485424, 0.2269161128669485, 0.2491216010045596, 0.27281273219022967, 0.2506218282845967, 0.26102314535949145, 0.24459295195483036, 0.2548871398765087, 0.22171013726917285, 0.23033231894551184, 0.22283445056827847, 0.2291326968353773, 0.2126270210790564, 0.2247300104707667, 0.22560368071828082, 0.22782250840819462, 0.23633502844112853, 0.24295643452671803, 0.23214751950415854, 0.23745356410521548, 0.21989314028982684, 0.22278734526664157, 0.22162254296865236, 0.23105509948744699, 0.2424532781346321, 0.21952886518734918, 0.23578586823543377, 0.22454951151557956, 0.2357542313218365, 0.23498964820670065, 0.2229408097329009, 0.24402888826600702, 0.23677317123624717, 0.2508951877904754, 0.22735500738991724, 0.24256303854912545, 0.22074824993058326]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "train_feature_means_minmax = []\n",
    "\n",
    "# Reshaping to 2-dimensional data for imputation\n",
    "X_train_2d_minmax = np.reshape(X_train_minmax, (X_train_minmax.shape[0]*X_train_minmax.shape[1], X_train_minmax.shape[2]))\n",
    "\n",
    "\n",
    "train_mean_imputer_minmax = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "train_mean_imputer_minmax.fit(X_train_2d_minmax)\n",
    "X_train_2d_minmax = train_mean_imputer_minmax.transform(X_train_2d_minmax)\n",
    "\n",
    "\n",
    "for i in range(X_train_2d.shape[1]):\n",
    "    train_feature_means_minmax.append(np.mean(X_train_2d_minmax[:][i]))\n",
    "\n",
    "\n",
    "\n",
    "print(train_feature_means_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "8qPYEc7B3GNQ"
   },
   "outputs": [],
   "source": [
    "def create_mean_imputed_data(X_train, X_test, train_feature_means, test_feature_means):\n",
    "    impute_value = 0.\n",
    "\n",
    "    X_train_imputed = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    X_test_imputed = np.empty([X_test.shape[0], X_test.shape[1], X_test.shape[2]])\n",
    "\n",
    "    train_mask = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    test_mask = np.empty([X_test.shape[0], X_test.shape[1], X_test.shape[2]])\n",
    "\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[1]):\n",
    "            for k in range(X_train.shape[2]):\n",
    "                if np.isnan(X_train[i,j,k]):\n",
    "                    X_train_imputed[i,j,k] = train_feature_means[k]\n",
    "                    train_mask[i,j,k] = 0\n",
    "                else:\n",
    "                    X_train_imputed[i,j,k] = X_train[i,j,k]\n",
    "                    train_mask[i,j,k] = 1\n",
    "\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        for j in range(X_test.shape[1]):\n",
    "            for k in range(X_test.shape[2]):\n",
    "                if np.isnan(X_test[i,j,k]):\n",
    "                    X_test_imputed[i,j,k] = train_feature_means[k]\n",
    "                    test_mask[i,j,k] = 0\n",
    "                else:\n",
    "                    X_test_imputed[i,j,k] = X_test[i,j,k]\n",
    "                    test_mask[i,j,k] = 1\n",
    "\n",
    "    return X_train_imputed, X_test_imputed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iComzTNR4cbF"
   },
   "outputs": [],
   "source": [
    "X_train_mean_imputed, X_test_mean_imputed = create_mean_imputed_data(X_train_minmax, X_test_minmax, \n",
    "                                                                     train_feature_means_minmax, train_feature_means_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xRwIrvuZ-M8W"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "n_est = 8\n",
    "max_depth = 25\n",
    "\n",
    "estimator =  RandomForestRegressor(n_estimators=n_est, max_depth=max_depth, bootstrap=True, \n",
    "                                  random_state=random_seed)\n",
    "X_train_2d = np.reshape(X_train_minmax, (X_train_minmax.shape[0]*X_train_minmax.shape[1], X_train_minmax.shape[2]))\n",
    "\n",
    "miss_forest_imputer = IterativeImputer(random_state=random_seed, estimator=estimator, max_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeI2wnkE-8Uv",
    "outputId": "a052c518-f3ec-49af-9791-edc35f971ade"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/c/clearloveyanzhen/miniconda3/envs/jeroda7105/lib/python3.9/site-packages/sklearn/impute/_iterative.py:700: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IterativeImputer(estimator=RandomForestRegressor(max_depth=25, n_estimators=8,\n",
       "                                                 random_state=33),\n",
       "                 random_state=33)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IterativeImputer</label><div class=\"sk-toggleable__content\"><pre>IterativeImputer(estimator=RandomForestRegressor(max_depth=25, n_estimators=8,\n",
       "                                                 random_state=33),\n",
       "                 random_state=33)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=25, n_estimators=8, random_state=33)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=25, n_estimators=8, random_state=33)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IterativeImputer(estimator=RandomForestRegressor(max_depth=25, n_estimators=8,\n",
       "                                                 random_state=33),\n",
       "                 random_state=33)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_forest_imputer.fit(X_train_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mf_imputed_data(X_train, X_test, miss_forest_impute):\n",
    "    \n",
    "\n",
    "    X_train_2d = np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    X_train_imputed_2d = miss_forest_imputer.transform(X_train_2d)\n",
    "    X_train_imputed = np.reshape(X_train_imputed_2d, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    \n",
    "\n",
    "    X_test_2d = np.reshape(X_test, (X_test.shape[0]*X_test.shape[1], X_test.shape[2])) \n",
    "\n",
    "    X_test_imputed_2d = miss_forest_imputer.transform(X_test_2d)\n",
    "    X_test_imputed = np.reshape(X_test_imputed_2d, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "    return X_train_imputed, X_test_imputed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mf_imputed, X_test_mf_imputed = create_mf_imputed_data(X_train, X_test, miss_forest_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "d9GnUNeC40Em"
   },
   "outputs": [],
   "source": [
    "def vae_preprocessing(X_train, X_test):\n",
    "    impute_value = 0.\n",
    "\n",
    "    X_train_imputed = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    X_test_imputed = np.empty([X_test.shape[0], X_test.shape[1], X_test.shape[2]])\n",
    "\n",
    "    train_mask = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    test_mask = np.empty([X_test.shape[0], X_test.shape[1], X_test.shape[2]])\n",
    "\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[1]):\n",
    "            for k in range(X_train.shape[2]):\n",
    "                if np.isnan(X_train[i,j,k]):\n",
    "                    X_train_imputed[i,j,k] = impute_value\n",
    "                    train_mask[i,j,k] = 0\n",
    "                else:\n",
    "                    X_train_imputed[i,j,k] = X_train[i,j,k]\n",
    "                    train_mask[i,j,k] = 1\n",
    "\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        for j in range(X_test.shape[1]):\n",
    "            for k in range(X_test.shape[2]):\n",
    "                if np.isnan(X_test[i,j,k]):\n",
    "                    X_test_imputed[i,j,k] = impute_value\n",
    "                    test_mask[i,j,k] = 0\n",
    "                else:\n",
    "                    X_test_imputed[i,j,k] = X_test[i,j,k]\n",
    "                    test_mask[i,j,k] = 1\n",
    "                    \n",
    "    return X_train_imputed, X_test_imputed, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "HPBch8O_5uFF"
   },
   "outputs": [],
   "source": [
    "processed_X_train, processed_X_test, train_mask, test_mask = vae_preprocessing(X_train_minmax, X_test_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Co301rq5CDqP"
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class vae_model(ABC):\n",
    "\n",
    "    def __init__(self, n_filters, kernel_size, learning_rate,\n",
    "               sequence_length, n_features):\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.latent_dim = 2\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_features = n_features\n",
    "    \n",
    "\n",
    "        if self.kernel_size == 3:\n",
    "            self.nn_dim = 21\n",
    "        elif self.kernel_size == 5:\n",
    "            self.nn_dim = 18\n",
    "        else:\n",
    "            self.kernel_size = 3\n",
    "            self.nn_dim = 21\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "    \n",
    "        tf.random.set_seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def sampling(self, args):\n",
    "      \n",
    "        latent_dim = 2\n",
    "        z_mean, z_log_sigma = args\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1)\n",
    "\n",
    "        return z_mean + K.exp(0.5 * z_log_sigma) * epsilon\n",
    "\n",
    "\n",
    "    def vae_loss(self, inp, mask, out, z_log_sigma, z_mean):\n",
    "        masked_input = tf.math.multiply(inp, mask)\n",
    "        masked_output = tf.math.multiply(out, mask)\n",
    "\n",
    "        #mse = np.sum(np.square(np.subtract(masked_output, masked_input))) / np.sum(mask)\n",
    "        mse = K.sum(K.square(masked_output - masked_input)) / K.sum(mask)\n",
    "\n",
    "        reconstruction = mse * self.sequence_length\n",
    "        kl = -0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "\n",
    "        return reconstruction + kl\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_model(self):\n",
    "        pass\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ev9NJEg-58HO"
   },
   "outputs": [],
   "source": [
    "class cnn_vae(vae_model):\n",
    "\n",
    "    def get_model(self):\n",
    "  \n",
    "\n",
    "        self.set_seed(random_seed)\n",
    "\n",
    "        # encoder\n",
    "\n",
    "        inp = tf.keras.Input(shape=(self.sequence_length, self.n_features))\n",
    "        mask = tf.keras.Input(shape=(self.sequence_length, self.n_features))\n",
    "\n",
    "\n",
    "        conv = tf.keras.layers.Conv1D(filters = self.n_filters, kernel_size = self.kernel_size, activation='relu')(inp)\n",
    "        print(conv.shape)\n",
    "\n",
    "        max_pool = tf.keras.layers.MaxPool1D(pool_size = 2)(conv) \n",
    "\n",
    "        conv = tf.keras.layers.Conv1D(filters = self.n_filters/2, kernel_size = self.kernel_size, activation='relu')(max_pool)\n",
    "        print(conv.shape)\n",
    "\n",
    "        enc = tf.keras.layers.Flatten()(conv)\n",
    "\n",
    "        enc = tf.keras.layers.Dense(self.nn_dim*8, activation=\"relu\")(enc)\n",
    "\n",
    "        enc = tf.keras.layers.Dense(self.nn_dim*4, activation=\"relu\")(enc)\n",
    "\n",
    "        enc = tf.keras.layers.Dense(self.nn_dim*2, activation=\"relu\")(enc)\n",
    "\n",
    "        z = tf.keras.layers.Dense(self.nn_dim, activation=\"relu\")(enc)\n",
    "\n",
    "        z_mean = tf.keras.layers.Dense(self.latent_dim)(z)\n",
    "        z_log_sigma = tf.keras.layers.Dense(self.latent_dim)(z)\n",
    "\n",
    "        encoder = tf.keras.Model([inp], [z_mean, z_log_sigma])\n",
    "\n",
    "        # decoder\n",
    "\n",
    "        inp_z = tf.keras.Input(shape=(self.latent_dim,))\n",
    "\n",
    "        dec = tf.keras.layers.Dense(self.nn_dim)(inp_z)\n",
    "\n",
    "        dec = tf.keras.layers.Dense(self.nn_dim*2)(dec)\n",
    "\n",
    "        dec = tf.keras.layers.Dense(self.nn_dim*4)(dec)\n",
    "\n",
    "        dec = tf.keras.layers.Dense(self.nn_dim*8)(dec)\n",
    "\n",
    "        dec = tf.keras.layers.Reshape((self.nn_dim, 8))(dec)\n",
    "\n",
    "        deconv = tf.keras.layers.Conv1DTranspose(filters=self.n_filters/2, kernel_size=self.kernel_size)(dec)\n",
    "        print(deconv.shape)\n",
    "\n",
    "        upsample = tf.keras.layers.UpSampling1D(2)(deconv)\n",
    "\n",
    "        deconv = tf.keras.layers.Conv1DTranspose(filters=self.n_features, kernel_size=self.kernel_size)(upsample)\n",
    "        print(deconv.shape)\n",
    "\n",
    "\n",
    "        out = deconv\n",
    "\n",
    "\n",
    "        decoder = tf.keras.Model([inp_z], out) \n",
    "\n",
    "        # encoder and decoder \n",
    "\n",
    "        z_mean, z_log_sigma = encoder([inp])\n",
    "        z = tf.keras.layers.Lambda(self.sampling)([z_mean, z_log_sigma])\n",
    "        pred = decoder([z])\n",
    "\n",
    "        vae = tf.keras.Model([inp,  mask], pred)\n",
    "        vae.add_loss(self.vae_loss(inp, mask, pred, z_log_sigma, z_mean))\n",
    "        vae.compile(loss=None, optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "dFC9RrY09eeL"
   },
   "outputs": [],
   "source": [
    "class lstm_vae(vae_model):\n",
    "    \n",
    "    def get_model(self):\n",
    "      \n",
    "        self.set_seed(random_seed)\n",
    "\n",
    "        # encoder\n",
    "\n",
    "        inp = tf.keras.Input(shape=(self.sequence_length, self.n_features))\n",
    "        mask = tf.keras.Input(shape=(self.sequence_length, self.n_features))\n",
    "\n",
    "\n",
    "        enc = tf.keras.layers.LSTM(192, input_shape=(self.sequence_length, self.n_features))(inp)\n",
    "\n",
    "        z = tf.keras.layers.Dense(96, activation=\"relu\")(enc)\n",
    "\n",
    "        z_mean = tf.keras.layers.Dense(self.latent_dim)(z)\n",
    "        z_log_sigma = tf.keras.layers.Dense(self.latent_dim)(z)\n",
    "\n",
    "        encoder = tf.keras.Model([inp], [z_mean, z_log_sigma])\n",
    "\n",
    "        # decoder\n",
    "\n",
    "        inp_z = tf.keras.Input(shape=(self.latent_dim,))\n",
    "\n",
    "        dec = tf.keras.layers.RepeatVector(self.sequence_length)(inp_z)\n",
    "\n",
    "        dec = tf.keras.layers.LSTM(192, input_shape=(self.sequence_length, self.n_features), return_sequences=True)(dec)\n",
    "\n",
    "        out = tf.keras.layers.TimeDistributed(Dense(self.n_features))(dec)\n",
    "\n",
    "        decoder = tf.keras.Model([inp_z], out) \n",
    "\n",
    "        # encoder and decoder \n",
    "\n",
    "        z_mean, z_log_sigma = encoder([inp])\n",
    "        z = tf.keras.layers.Lambda(self.sampling)([z_mean, z_log_sigma])\n",
    "        pred = decoder([z])\n",
    "\n",
    "        vae = tf.keras.Model([inp,  mask], pred)\n",
    "        vae.add_loss(self.vae_loss(inp, mask, pred, z_log_sigma, z_mean))\n",
    "        vae.compile(loss=None, optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        return vae\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "pwKhQRmD8sWn"
   },
   "outputs": [],
   "source": [
    "def train_eval_vae_model(model, processed_X_train, processed_X_test, train_mask, test_mask, batch_size):\n",
    "  \n",
    "    es = EarlyStopping(patience=10, verbose=1, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "    model.fit([processed_X_train, train_mask], batch_size=batch_size, validation_split=0.2, epochs=100, shuffle=False, callbacks=[es])\n",
    "\n",
    "\n",
    "    vae = tf.keras.Model(model.input, model.output)\n",
    "\n",
    "    reconstruc_train = vae.predict([processed_X_train,  train_mask])\n",
    "    reconstruc_test = vae.predict([processed_X_test, test_mask])\n",
    "\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "\n",
    "    #print(mask_X_test_imputed[1])\n",
    "    masked_reconstruction = tf.math.multiply(reconstruc_test, test_mask)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(processed_X_test.shape[0]):\n",
    "        mse += np.sum(np.square(np.subtract(processed_X_test[i], masked_reconstruction[i]))) / np.sum(test_mask[i])\n",
    "        mae += np.sum(np.absolute(np.subtract(processed_X_test[i], masked_reconstruction[i]))) / np.sum(test_mask[i])\n",
    "\n",
    "\n",
    "    print(\"test mse: \", mse / processed_X_test.shape[0])\n",
    "    print(\"test mae: \", mae / processed_X_test.shape[0], \"\\n\")\n",
    "\n",
    "    return model, reconstruc_train, reconstruc_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "E5FkSre_-p9t"
   },
   "outputs": [],
   "source": [
    "def all_eval(X, X_minmax, scalers, test_minmax_scalers, cnn_vae, lstm_vae, batch_size, miss_forest_imputer, train_feature_means, \n",
    "             knn_imputers, lstm_ae_model, mask_prop):\n",
    "    # Creating a random mask for evaluation of imputation \n",
    "\n",
    "    iter = 30\n",
    "    cnn_vae_mse_list = []\n",
    "    cnn_vae_mae_list = []\n",
    "\n",
    "    lstm_vae_mse_list = []\n",
    "    lstm_vae_mae_list = []\n",
    "    \n",
    "    mf_mse_list = []\n",
    "    mf_mae_list = []\n",
    "    \n",
    "    mean_mse_list = []\n",
    "    mean_mae_list = []\n",
    "    \n",
    "    fill_mse_list = []\n",
    "    fill_mae_list = []\n",
    "    \n",
    "    dynimp_mse_list = []\n",
    "    dynimp_mae_list = []\n",
    "    \n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    rand_X_imputed = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "    mask_X_imputed = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "    rand_mask = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "\n",
    "    for j in range(iter):\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            rand_mask_1d = np.random.choice([0,1], size=X.shape[1]*X.shape[2], replace=True, p=[mask_prop, 1-mask_prop])\n",
    "            rand_mask[i] = np.reshape(rand_mask_1d, (X.shape[1],X.shape[2]))\n",
    "\n",
    "        rand_mask = np.where(np.isnan(X), 0, rand_mask)\n",
    "\n",
    "        cnn_vae_mse = 0\n",
    "        cnn_vae_mae = 0\n",
    "\n",
    "        lstm_vae_mse = 0\n",
    "        lstm_vae_mae = 0\n",
    "        \n",
    "        mf_mse = 0\n",
    "        mf_mae = 0\n",
    "        \n",
    "        mean_mse = 0\n",
    "        mean_mae = 0\n",
    "        \n",
    "        fill_mse = 0\n",
    "        fill_mae = 0\n",
    "        \n",
    "        dynimp_mse = 0\n",
    "        dynimp_mae = 0\n",
    "\n",
    "        # Actual mask of observations for comparison\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        ############################################ \n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, 0, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        \n",
    "        # Performing and evaluating cnn_vae imputation\n",
    "        cnn_vae_imputated_data = cnn_vae.predict([rand_X_imputed, rand_mask], batch_size=batch_size)\n",
    "\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        cnn_vae_imputated_data = np.where(rand_mask==0, cnn_vae_imputated_data, 0)\n",
    "        cnn_vae_imputated_data = np.where(np.isnan(X), 0, cnn_vae_imputated_data)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            cnn_vae_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], cnn_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "            cnn_vae_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], cnn_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "        cnn_vae_mse_list.append(cnn_vae_mse / n_samples)\n",
    "        cnn_vae_mae_list.append(cnn_vae_mae / n_samples)\n",
    "\n",
    "        ############################################ \n",
    "        \n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "        # lstm_vae imputation\n",
    "        rand_X_imputed = np.where(rand_mask==0, 0, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        \n",
    "        # Performing and evaluating lstm_vae imputation\n",
    "        lstm_vae_imputated_data = lstm_vae.predict([rand_X_imputed, rand_mask], batch_size=batch_size)\n",
    "\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        lstm_vae_imputated_data = np.where(rand_mask==0, lstm_vae_imputated_data, 0)\n",
    "        lstm_vae_imputated_data = np.where(np.isnan(X), 0, lstm_vae_imputated_data)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            lstm_vae_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], lstm_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "            lstm_vae_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], lstm_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "        lstm_vae_mse_list.append(lstm_vae_mse / n_samples)\n",
    "        lstm_vae_mae_list.append(lstm_vae_mae / n_samples)\n",
    "        \n",
    "        ############################################ \n",
    "        \n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        \n",
    "        # Performing and evaluating missforest imputation\n",
    "        \n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        # imputing on the random mask data\n",
    "        rand_X_imputed_2d = np.reshape(rand_X_imputed, (rand_X_imputed.shape[0]*rand_X_imputed.shape[1], rand_X_imputed.shape[2]))\n",
    "\n",
    "        mf_X_imputed_2d = miss_forest_imputer.transform(rand_X_imputed_2d)\n",
    "        mf_X_imputed = np.reshape(mf_X_imputed_2d, (rand_X_imputed.shape[0], rand_X_imputed.shape[1], rand_X_imputed.shape[2]))\n",
    "\n",
    "        #print(rand_X_test_imputed)\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        mf_X_imputed = np.where(rand_mask==0, mf_X_imputed, 0)\n",
    "        mf_X_imputed = np.where(np.isnan(X), 0, mf_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            mf_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], mf_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            mf_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], mf_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        mf_mse_list.append(mf_mse / n_samples)\n",
    "        mf_mae_list.append(mf_mae / n_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################ \n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        \n",
    "        # Performing and evaluating mean imputation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        mean_X_imputed = np.where(np.isnan(rand_X_imputed), train_feature_means,  rand_X_imputed)\n",
    "        \n",
    "        \n",
    "        mean_X_imputed = np.where(rand_mask==0, mean_X_imputed, 0)\n",
    "        mean_X_imputed = np.where(np.isnan(X), 0, mean_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            mean_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], mean_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            mean_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], mean_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        mean_mse_list.append(mean_mse / n_samples)\n",
    "        mean_mae_list.append(mean_mae / n_samples)\n",
    "\n",
    "        ############################################ \n",
    "         # Actual mask of observations for comparison\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        # imputing on the random mask data\n",
    "        \n",
    "        for k in range(rand_X_imputed.shape[0]):\n",
    "            intermediate_df = pd.DataFrame(rand_X_imputed[k,:,:])\n",
    "            intermediate_df = intermediate_df.ffill()\n",
    "            intermediate_df = intermediate_df.bfill()\n",
    "            intermediate_array = intermediate_df.to_numpy()\n",
    "            rand_X_imputed[k,:,:] = intermediate_array\n",
    "        \n",
    "        forward_X_imputed = np.where(np.isnan(rand_X_imputed), train_feature_means,  rand_X_imputed)\n",
    "\n",
    "        #print(rand_X_test_imputed)\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        forward_X_imputed = np.where(rand_mask==0, forward_X_imputed, 0)\n",
    "        forward_X_imputed = np.where(np.isnan(X), 0, forward_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            fill_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], forward_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            fill_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], forward_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        fill_mse_list.append(fill_mse / n_samples)\n",
    "        fill_mae_list.append(fill_mae / n_samples)\n",
    "\n",
    "        ############################################ \n",
    "         # Actual mask of observations for comparison\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        # imputing on the random mask data\n",
    "        \n",
    "        rand_X_imputed_knn = np.empty([rand_X_imputed.shape[0], rand_X_imputed.shape[1], rand_X_imputed.shape[2]])\n",
    "        rand_X_imputed_transposed = rand_X_imputed.transpose((1, 0, 2))\n",
    "\n",
    "        for i in range(rand_X_imputed_transposed.shape[0]):\n",
    "\n",
    "            cur_imputer = knn_imputers[i]\n",
    "\n",
    "            rand_X_imputed_transposed[i] = cur_imputer.transform(rand_X_imputed_transposed[i])\n",
    "\n",
    "        rand_X_imputed_knn = rand_X_imputed_transposed.transpose((1, 0, 2))\n",
    "\n",
    "        dynimp_X_imputed = lstm_ae_model.predict(rand_X_imputed_knn, batch_size=batch_size)\n",
    "        \n",
    "\n",
    "        #print(rand_X_test_imputed)\n",
    "        \n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        dynimp_X_imputed = np.where(rand_mask==0, dynimp_X_imputed, 0)\n",
    "        dynimp_X_imputed = np.where(np.isnan(X_minmax), 0, dynimp_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X_minmax), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X_minmax), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            dynimp_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], dynimp_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            dynimp_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], dynimp_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        dynimp_mse_list.append(dynimp_mse / n_samples)\n",
    "        dynimp_mae_list.append(dynimp_mae / n_samples)\n",
    "\n",
    "    print(\"fill mse:\")\n",
    "    print(\"mean: \", np.mean(fill_mse_list))\n",
    "    print(\"std dev: \",np.std(fill_mse_list), \"\\n\")\n",
    "\n",
    "\n",
    "    print(\"fill_mae:\")\n",
    "    print(\"mean: \", np.mean(fill_mae_list))\n",
    "    print(\"std dev: \",np.std(fill_mae_list), \"\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    print(\"cnn-vae imputation mse:\")\n",
    "    print(\"mean: \", np.mean(cnn_vae_mse_list))\n",
    "    print(\"std dev: \", np.std(cnn_vae_mse_list), \"\\n\")\n",
    "\n",
    "    print(\"cnn-vae imputation mae:\")\n",
    "    print(\"mean: \", np.mean(cnn_vae_mae_list))\n",
    "    print(\"std dev: \", np.std(cnn_vae_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"lstm-vae imputation mse:\")\n",
    "    print(\"mean: \", np.mean(lstm_vae_mse_list))\n",
    "    print(\"std dev: \", np.std(lstm_vae_mse_list), \"\\n\")\n",
    "\n",
    "    print(\"lstm-vae imputation mae:\")\n",
    "    print(\"mean: \", np.mean(lstm_vae_mae_list))\n",
    "    print(\"std dev: \", np.std(lstm_vae_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "    print(\"missforest imputation mse:\")\n",
    "    print(\"mean: \", np.mean(mf_mse_list))\n",
    "    print(\"std dev: \", np.std(mf_mse_list), \"\\n\")\n",
    "    \n",
    "    print(\"missforest imputation mae:\")\n",
    "    print(\"mean: \", np.mean(mf_mae_list))\n",
    "    print(\"std dev: \", np.std(mf_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"mean imputation mse:\")\n",
    "    print(\"mean: \", np.mean(mean_mse_list))\n",
    "    print(\"std dev: \", np.std(mean_mse_list), \"\\n\")\n",
    "    \n",
    "    print(\"mean imputation mae:\")\n",
    "    print(\"mean: \", np.mean(mean_mae_list))\n",
    "    print(\"std dev: \", np.std(mean_mae_list), \"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"dynimp imputation mse:\")\n",
    "    print(\"mean: \", np.mean(dynimp_mse_list))\n",
    "    print(\"std dev: \", np.std(dynimp_mse_list), \"\\n\")\n",
    "    \n",
    "    print(\"dynimp imputation mae:\")\n",
    "    print(\"mean: \", np.mean(dynimp_mae_list))\n",
    "    print(\"std dev: \", np.std(dynimp_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhSubm4Lfm4J",
    "outputId": "cebf73c0-8259-4dbc-dde2-b1a52682a046"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 00:57:33.734510: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-04 00:57:33.734598: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: cse-stmi-s1.cse.tamu.edu\n",
      "2022-06-04 00:57:33.734620: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: cse-stmi-s1.cse.tamu.edu\n",
      "2022-06-04 00:57:33.734844: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.129.6\n",
      "2022-06-04 00:57:33.734935: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.129.6\n",
      "2022-06-04 00:57:33.734950: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.129.6\n",
      "2022-06-04 00:57:33.735626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 44, 32)\n",
      "(None, 18, 16)\n",
      "(None, 22, 16)\n",
      "(None, 48, 48)\n",
      "Epoch 1/100\n",
      "321/321 [==============================] - 3s 5ms/step - loss: 7.3585 - val_loss: 5.0330\n",
      "Epoch 2/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 4.0451 - val_loss: 3.5097\n",
      "Epoch 3/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 3.3190 - val_loss: 3.2591\n",
      "Epoch 4/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 3.1689 - val_loss: 3.1937\n",
      "Epoch 5/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 3.1007 - val_loss: 3.1314\n",
      "Epoch 6/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 3.0438 - val_loss: 3.0565\n",
      "Epoch 7/100\n",
      "321/321 [==============================] - 1s 3ms/step - loss: 3.0193 - val_loss: 3.0475\n",
      "Epoch 8/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.9837 - val_loss: 2.9684\n",
      "Epoch 9/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.9586 - val_loss: 2.9839\n",
      "Epoch 10/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.9178 - val_loss: 2.9636\n",
      "Epoch 11/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.9021 - val_loss: 2.9213\n",
      "Epoch 12/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.8726 - val_loss: 2.8581\n",
      "Epoch 13/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7856 - val_loss: 2.8760\n",
      "Epoch 14/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7825 - val_loss: 2.7972\n",
      "Epoch 15/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7665 - val_loss: 2.8179\n",
      "Epoch 16/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7824 - val_loss: 2.8022\n",
      "Epoch 17/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7634 - val_loss: 2.7643\n",
      "Epoch 18/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7145 - val_loss: 2.7606\n",
      "Epoch 19/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6791 - val_loss: 2.7300\n",
      "Epoch 20/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6676 - val_loss: 2.7240\n",
      "Epoch 21/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7129 - val_loss: 2.7354\n",
      "Epoch 22/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7181 - val_loss: 2.6800\n",
      "Epoch 23/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.7028 - val_loss: 2.7020\n",
      "Epoch 24/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6760 - val_loss: 2.8098\n",
      "Epoch 25/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6603 - val_loss: 2.7282\n",
      "Epoch 26/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6850 - val_loss: 2.7009\n",
      "Epoch 27/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6715 - val_loss: 2.6932\n",
      "Epoch 28/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6466 - val_loss: 2.7556\n",
      "Epoch 29/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6450 - val_loss: 2.7391\n",
      "Epoch 30/100\n",
      "321/321 [==============================] - 1s 3ms/step - loss: 2.6226 - val_loss: 2.6685\n",
      "Epoch 31/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6232 - val_loss: 2.6157\n",
      "Epoch 32/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6321 - val_loss: 2.6180\n",
      "Epoch 33/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6664 - val_loss: 2.6731\n",
      "Epoch 34/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6539 - val_loss: 2.6931\n",
      "Epoch 35/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6361 - val_loss: 2.6154\n",
      "Epoch 36/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6378 - val_loss: 2.6104\n",
      "Epoch 37/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6136 - val_loss: 2.7141\n",
      "Epoch 38/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6492 - val_loss: 2.6329\n",
      "Epoch 39/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6268 - val_loss: 2.6804\n",
      "Epoch 40/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6343 - val_loss: 2.6328\n",
      "Epoch 41/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6320 - val_loss: 2.6333\n",
      "Epoch 42/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6147 - val_loss: 2.7142\n",
      "Epoch 43/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.5876 - val_loss: 2.6927\n",
      "Epoch 44/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6076 - val_loss: 2.6990\n",
      "Epoch 45/100\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6180 - val_loss: 2.6647\n",
      "Epoch 46/100\n",
      "311/321 [============================>.] - ETA: 0s - loss: 2.6385Restoring model weights from the end of the best epoch: 36.\n",
      "321/321 [==============================] - 1s 4ms/step - loss: 2.6270 - val_loss: 2.7219\n",
      "Epoch 46: early stopping\n",
      "13/13 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "test mse:  0.04634742398873441\n",
      "test mae:  0.11874298388374861 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_vae_instance = cnn_vae(n_filters=32, kernel_size=5, learning_rate=1e-4, \n",
    "                                    sequence_length=48, n_features=48)\n",
    "\n",
    "\n",
    "cnn_vae_model = cnn_vae_instance.get_model()\n",
    "\n",
    "trained_cnn_vae_model, cnn_reconstruc_train, cnn_reconstruc_test = train_eval_vae_model(cnn_vae_model, \n",
    "                                                processed_X_train, processed_X_test, train_mask, test_mask, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6x6arDB9o6E",
    "outputId": "002eb62c-63d0-4237-a464-7e2e19b07b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "321/321 [==============================] - 16s 39ms/step - loss: 4.8964 - val_loss: 3.9339\n",
      "Epoch 2/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 3.8116 - val_loss: 3.5857\n",
      "Epoch 3/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 3.5069 - val_loss: 3.3269\n",
      "Epoch 4/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 3.2016 - val_loss: 3.1653\n",
      "Epoch 5/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 3.0611 - val_loss: 3.1001\n",
      "Epoch 6/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.9505 - val_loss: 2.9710\n",
      "Epoch 7/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.9190 - val_loss: 3.0197\n",
      "Epoch 8/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.8221 - val_loss: 2.7964\n",
      "Epoch 9/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.8011 - val_loss: 2.8189\n",
      "Epoch 10/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7813 - val_loss: 2.8663\n",
      "Epoch 11/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7845 - val_loss: 2.8114\n",
      "Epoch 12/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7859 - val_loss: 2.8155\n",
      "Epoch 13/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7439 - val_loss: 2.7898\n",
      "Epoch 14/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7379 - val_loss: 2.7474\n",
      "Epoch 15/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7196 - val_loss: 2.7920\n",
      "Epoch 16/100\n",
      "321/321 [==============================] - 11s 34ms/step - loss: 2.7681 - val_loss: 2.7488\n",
      "Epoch 17/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7314 - val_loss: 2.7407\n",
      "Epoch 18/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6875 - val_loss: 2.7116\n",
      "Epoch 19/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6676 - val_loss: 2.6850\n",
      "Epoch 20/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6383 - val_loss: 2.7059\n",
      "Epoch 21/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6942 - val_loss: 2.7124\n",
      "Epoch 22/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.7122 - val_loss: 2.6559\n",
      "Epoch 23/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6815 - val_loss: 2.6683\n",
      "Epoch 24/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6558 - val_loss: 2.7699\n",
      "Epoch 25/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6331 - val_loss: 2.6912\n",
      "Epoch 26/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6628 - val_loss: 2.6835\n",
      "Epoch 27/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6540 - val_loss: 2.6680\n",
      "Epoch 28/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6331 - val_loss: 2.7725\n",
      "Epoch 29/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6281 - val_loss: 2.7193\n",
      "Epoch 30/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6135 - val_loss: 2.6849\n",
      "Epoch 31/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.5908 - val_loss: 2.5986\n",
      "Epoch 32/100\n",
      "321/321 [==============================] - 11s 36ms/step - loss: 2.5928 - val_loss: 2.5793\n",
      "Epoch 33/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6334 - val_loss: 2.6285\n",
      "Epoch 34/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6240 - val_loss: 2.6668\n",
      "Epoch 35/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6053 - val_loss: 2.5912\n",
      "Epoch 36/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.5895 - val_loss: 2.5746\n",
      "Epoch 37/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.5953 - val_loss: 2.7381\n",
      "Epoch 38/100\n",
      "321/321 [==============================] - 11s 36ms/step - loss: 2.5959 - val_loss: 2.6233\n",
      "Epoch 39/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.5969 - val_loss: 2.6796\n",
      "Epoch 40/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6104 - val_loss: 2.5914\n",
      "Epoch 41/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.6002 - val_loss: 2.6255\n",
      "Epoch 42/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.5817 - val_loss: 2.7220\n",
      "Epoch 43/100\n",
      "321/321 [==============================] - 11s 35ms/step - loss: 2.5725 - val_loss: 2.6762\n",
      "Epoch 44/100\n",
      "321/321 [==============================] - 12s 36ms/step - loss: 2.5796 - val_loss: 2.7166\n",
      "Epoch 45/100\n",
      "321/321 [==============================] - 12s 36ms/step - loss: 2.5819 - val_loss: 2.6659\n",
      "Epoch 46/100\n",
      "321/321 [==============================] - ETA: 0s - loss: 2.5784Restoring model weights from the end of the best epoch: 36.\n",
      "321/321 [==============================] - 11s 36ms/step - loss: 2.5784 - val_loss: 2.6856\n",
      "Epoch 46: early stopping\n",
      "13/13 [==============================] - 1s 41ms/step\n",
      "4/4 [==============================] - 0s 34ms/step\n",
      "test mse:  0.04439292563226151\n",
      "test mae:  0.1096355796685531 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_vae_instance = lstm_vae(n_filters=32, kernel_size=5, learning_rate=1e-4, \n",
    "                                    sequence_length=48, n_features=48)\n",
    "\n",
    "\n",
    "lstm_vae_model = lstm_vae_instance.get_model()\n",
    "\n",
    "trained_lstm_vae_model, lstm_reconstruc_train, lstm_reconstruc_test = train_eval_vae_model(lstm_vae_model, \n",
    "                                                processed_X_train, processed_X_test, train_mask, test_mask, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynimp\n",
    "\n",
    "# Create train set with noise\n",
    "\n",
    "def create_X_train_w_noise(X_train, mask_prop):\n",
    "    \n",
    "    X_train_noise = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    rand_mask = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    \n",
    "    for i in range(X_train.shape[0]):\n",
    "        rand_mask_1d = np.random.choice([0,1], size=X_train.shape[1]*X_train.shape[2], replace=True, p=[mask_prop, 1-mask_prop])\n",
    "        rand_mask[i] = np.reshape(rand_mask_1d, (X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    rand_mask = np.where(np.isnan(X_train), 0, rand_mask)\n",
    "             \n",
    "    X_train_noise = np.where(rand_mask==0, np.nan, X_train)\n",
    "    \n",
    "    return X_train_noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w_noise = create_X_train_w_noise(X_train_minmax, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KNN on train set with noise\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "knn_imputers = {}\n",
    "\n",
    "X_train_w_noise_transposed = X_train_w_noise.transpose((1, 0, 2))\n",
    "\n",
    "for i in range(X_train_w_noise_transposed.shape[0]):\n",
    "    \n",
    "    cur_imputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    X_train_w_noise_transposed[i] = cur_imputer.fit_transform(X_train_w_noise_transposed[i])\n",
    "    \n",
    "    knn_imputers[i] = cur_imputer\n",
    "    \n",
    "\n",
    "X_train_noise = X_train_w_noise_transposed.transpose((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KNN on train set with noise\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_transposed = X_train_minmax.transpose((1, 0, 2))\n",
    "\n",
    "for i in range(X_train_transposed.shape[0]):\n",
    "    \n",
    "    cur_imputer = knn_imputers[i]\n",
    "    \n",
    "    X_train_transposed[i] = cur_imputer.transform(X_train_transposed[i])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "X_train_knn = X_train_transposed.transpose((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38351648 0.47367301 0.37433809 ... 0.         1.         0.44444444]\n",
      " [0.40776699 0.37142857 0.26582011 ... 0.         1.         0.        ]\n",
      " [0.21527778 0.14473684 0.35889831 ... 0.         1.         0.        ]\n",
      " ...\n",
      " [0.4        0.5        0.29971989 ... 0.         1.         0.        ]\n",
      " [0.66666667 0.5        0.27113703 ... 0.         1.         0.        ]\n",
      " [0.32380952 0.5        0.34005764 ... 0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_knn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38351648 0.55323902 0.36171079 ... 0.         1.         0.59460317]\n",
      " [0.40776699 0.42857143 0.30582011 ... 0.         1.         0.        ]\n",
      " [0.21527778 0.14473684 0.35889831 ... 0.         1.         0.        ]\n",
      " ...\n",
      " [0.4        0.5        0.36582633 ... 0.         1.         0.        ]\n",
      " [0.69615385 0.5        0.27113703 ... 0.         1.         0.        ]\n",
      " [0.32380952 0.5        0.34005764 ... 0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_noise[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM autoencoder\n",
    "\n",
    "class lstm_ae():\n",
    "    \n",
    "    def __init__(self, learning_rate, sequence_length, n_features):\n",
    "        \n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_features = n_features\n",
    "        \n",
    "    \n",
    "    def ae_loss(self, inp, output, mask):\n",
    "        initializer = tf.keras.initializers.Ones()\n",
    "        ones = initializer(shape=(self.sequence_length, self.n_features))\n",
    "\n",
    "        loss = -1*((inp*K.log(output)) + ((ones - inp) * K.log(ones - output)))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def get_lstm_ae(self):\n",
    "\n",
    "\n",
    "        # Encoding   \n",
    "\n",
    "        inp = tf.keras.Input(shape=(self.sequence_length, self.n_features))\n",
    "        print(inp.shape)\n",
    "\n",
    "    \n",
    "        encoded = tf.keras.layers.LSTM(48, return_sequences=True)(inp)\n",
    "        encoded = tf.keras.layers.LSTM(32, return_sequences=True)(encoded)\n",
    "        # decoder\n",
    "        decoded = tf.keras.layers.LSTM(48, return_sequences=True)(encoded)\n",
    "\n",
    "        out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.n_features, activation='sigmoid'))(decoded)\n",
    "\n",
    "        lstm_ae = tf.keras.Model(inputs=inp, outputs=decoded)\n",
    "\n",
    "        #lstm_ae.add_loss(self.ae_loss(inp, decoded, mask))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        lstm_ae.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "\n",
    "        return lstm_ae\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_X_train_minmax = np.where(np.isnan(X_train_minmax), 0, X_train_minmax)\n",
    "processed_X_test_minmax = np.where(np.isnan(X_test_minmax), 0, X_test_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 48, 48)\n",
      "Epoch 1/100\n",
      "321/321 [==============================] - 19s 45ms/step - loss: 0.9330 - val_loss: 0.7580\n",
      "Epoch 2/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.7033 - val_loss: 0.6887\n",
      "Epoch 3/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.6720 - val_loss: 0.6666\n",
      "Epoch 4/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.6561 - val_loss: 0.6538\n",
      "Epoch 5/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.6141 - val_loss: 0.4438\n",
      "Epoch 6/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.4814 - val_loss: 0.4454\n",
      "Epoch 7/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.4368 - val_loss: 0.4389\n",
      "Epoch 8/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.4252 - val_loss: 0.4289\n",
      "Epoch 9/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.4177 - val_loss: 0.4228\n",
      "Epoch 10/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.4111 - val_loss: 0.4185\n",
      "Epoch 11/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.4043 - val_loss: 0.4136\n",
      "Epoch 12/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.3989 - val_loss: 0.4067\n",
      "Epoch 13/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.3932 - val_loss: 0.4018\n",
      "Epoch 14/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.3914 - val_loss: 0.3982\n",
      "Epoch 15/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.3844 - val_loss: 0.3937\n",
      "Epoch 16/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2955 - val_loss: 0.2558\n",
      "Epoch 17/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.2404 - val_loss: 0.2479\n",
      "Epoch 18/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.2345 - val_loss: 0.2430\n",
      "Epoch 19/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2327 - val_loss: 0.2402\n",
      "Epoch 20/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2286 - val_loss: 0.2578\n",
      "Epoch 21/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2279 - val_loss: 0.2349\n",
      "Epoch 22/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2235 - val_loss: 0.2325\n",
      "Epoch 23/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2227 - val_loss: 0.2310\n",
      "Epoch 24/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2192 - val_loss: 0.2287\n",
      "Epoch 25/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2168 - val_loss: 0.2265\n",
      "Epoch 26/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2154 - val_loss: 0.2250\n",
      "Epoch 27/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2133 - val_loss: 0.2234\n",
      "Epoch 28/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2114 - val_loss: 0.2213\n",
      "Epoch 29/100\n",
      "321/321 [==============================] - 13s 42ms/step - loss: 0.2098 - val_loss: 0.2197\n",
      "Epoch 30/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.2091 - val_loss: 0.2191\n",
      "Epoch 31/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2070 - val_loss: 0.2172\n",
      "Epoch 32/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2058 - val_loss: 0.2163\n",
      "Epoch 33/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2046 - val_loss: 0.2154\n",
      "Epoch 34/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.2045 - val_loss: 0.2151\n",
      "Epoch 35/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.2008 - val_loss: 0.2089\n",
      "Epoch 36/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1975 - val_loss: 0.2025\n",
      "Epoch 37/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1920 - val_loss: 0.1994\n",
      "Epoch 38/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1951 - val_loss: 0.1996\n",
      "Epoch 39/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1908 - val_loss: 0.1962\n",
      "Epoch 40/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1891 - val_loss: 0.1948\n",
      "Epoch 41/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1881 - val_loss: 0.1947\n",
      "Epoch 42/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1874 - val_loss: 0.1941\n",
      "Epoch 43/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1867 - val_loss: 0.2023\n",
      "Epoch 44/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1877 - val_loss: 0.1932\n",
      "Epoch 45/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1872 - val_loss: 0.1926\n",
      "Epoch 46/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1859 - val_loss: 0.1992\n",
      "Epoch 47/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1864 - val_loss: 0.1929\n",
      "Epoch 48/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1846 - val_loss: 0.1921\n",
      "Epoch 49/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1841 - val_loss: 0.1918\n",
      "Epoch 50/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1837 - val_loss: 0.1921\n",
      "Epoch 51/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1834 - val_loss: 0.1919\n",
      "Epoch 52/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1829 - val_loss: 0.1915\n",
      "Epoch 53/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.1823 - val_loss: 0.1921\n",
      "Epoch 54/100\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.1828 - val_loss: 0.1920\n",
      "Epoch 55/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1819 - val_loss: 0.1910\n",
      "Epoch 56/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1815 - val_loss: 0.1909\n",
      "Epoch 57/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.1810 - val_loss: 0.1905\n",
      "Epoch 58/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1806 - val_loss: 0.1905\n",
      "Epoch 59/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.1803 - val_loss: 0.1898\n",
      "Epoch 60/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1816 - val_loss: 0.1901\n",
      "Epoch 61/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1805 - val_loss: 0.1889\n",
      "Epoch 62/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1797 - val_loss: 0.1888\n",
      "Epoch 63/100\n",
      "321/321 [==============================] - 13s 42ms/step - loss: 0.1803 - val_loss: 0.1909\n",
      "Epoch 64/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1815 - val_loss: 0.1891\n",
      "Epoch 65/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1794 - val_loss: 0.1889\n",
      "Epoch 66/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1787 - val_loss: 0.1886\n",
      "Epoch 67/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1784 - val_loss: 0.1884\n",
      "Epoch 68/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1781 - val_loss: 0.1884\n",
      "Epoch 69/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1779 - val_loss: 0.1882\n",
      "Epoch 70/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.1777 - val_loss: 0.1884\n",
      "Epoch 71/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1775 - val_loss: 0.1884\n",
      "Epoch 72/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1773 - val_loss: 0.1884\n",
      "Epoch 73/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1771 - val_loss: 0.1885\n",
      "Epoch 74/100\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1827 - val_loss: 0.1905\n",
      "Epoch 75/100\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.1785 - val_loss: 0.1887\n",
      "Epoch 76/100\n",
      "321/321 [==============================] - ETA: 0s - loss: 0.1773Restoring model weights from the end of the best epoch: 66.\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.1773 - val_loss: 0.1889\n",
      "Epoch 76: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc41c63f070>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit LSTM autoencoder accounting for missing data\n",
    "\n",
    "lstm_ae_instance = lstm_ae(learning_rate=1e-3, sequence_length=48, n_features=48)\n",
    "lstm_ae_model = lstm_ae_instance.get_lstm_ae()\n",
    "\n",
    "es = EarlyStopping(patience=10, verbose=1, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "lstm_ae_model.fit(x=X_train_noise, y=X_train_knn, batch_size=1,\n",
    "                  validation_split=0.2, epochs=100, shuffle=False, callbacks=[es])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTmlJN7d-p9u",
    "outputId": "f3b2b81b-19b8-4d63-83cc-c6d0f22171a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 2s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.0016380032542041357\n",
      "std dev:  5.7231053494523e-05 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.004062203972215389\n",
      "std dev:  6.472689717730176e-05 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.004357889895943933\n",
      "std dev:  0.00010403397468513365 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.011564971219951542\n",
      "std dev:  0.00015499244001524246 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.004226051084512713\n",
      "std dev:  0.00010928716388545206 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.010845241462342652\n",
      "std dev:  0.00015702048413138172 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.0018687142127123758\n",
      "std dev:  7.879898268677943e-05 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.005040072862671329\n",
      "std dev:  0.0001118391073229926 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.011971020571454234\n",
      "std dev:  0.00011810403883832148 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.029020428181405534\n",
      "std dev:  0.0002184212022332112 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.003996640050024822\n",
      "std dev:  7.279185897171968e-05 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.009461300984565026\n",
      "std dev:  0.00013649040650772667 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "         miss_forest_imputer, train_feature_means_minmax, knn_imputers, lstm_ae_model,  0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hS5hfoZcC9sF",
    "outputId": "3e7b2d5b-f8c2-46fc-b5cd-c7a5217e5070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "fill mse:\n",
      "mean:  0.00347708353982388\n",
      "std dev:  7.331863791649135e-05 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.008372795774135518\n",
      "std dev:  8.390162517541625e-05 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.008752178591218277\n",
      "std dev:  0.00016904972530417965 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.023232139748154777\n",
      "std dev:  0.00024773812912631186 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.00854328589740691\n",
      "std dev:  0.00022263873014931248 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.021846239500853546\n",
      "std dev:  0.00028118723896899007 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.004640585441791577\n",
      "std dev:  0.00014235796706599192 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.011365605547253441\n",
      "std dev:  0.0001798981534210648 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.03649591164376311\n",
      "std dev:  0.000265895665918055 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.05257960056433187\n",
      "std dev:  0.00031852348833761896 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.00823035323970892\n",
      "std dev:  0.00011219297295591237 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.019391221323930674\n",
      "std dev:  0.0001701618372200805 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,  0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObW2y6E5C_kG",
    "outputId": "3c15230e-6cc4-4239-be5c-128ba152bc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.005575169738462967\n",
      "std dev:  0.00012320191623127637 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.013025647692745692\n",
      "std dev:  0.00014180312363368838 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.013427170215949486\n",
      "std dev:  0.00025731368298712726 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.035266493313049506\n",
      "std dev:  0.0004810529790825522 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.012897015192906149\n",
      "std dev:  0.00022445052085648173 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.032866101435289216\n",
      "std dev:  0.0003236203069867292 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.00838552438187668\n",
      "std dev:  0.0001785803098732669 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.019161872057672436\n",
      "std dev:  0.00022752163168519884 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.05472734026460564\n",
      "std dev:  0.00028879284332166533 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.07885195858036376\n",
      "std dev:  0.000334753806042135 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.012808888597085392\n",
      "std dev:  0.00011750998938186534 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.029946347305212603\n",
      "std dev:  0.00017480915630674652 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNzfgb0bDAc5",
    "outputId": "a9ca29f0-0625-48dd-bb47-4c4ee137e9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "fill mse:\n",
      "mean:  0.00796380275505679\n",
      "std dev:  0.00015997597268784262 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.01802041457165447\n",
      "std dev:  0.00016066926064196437 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.018201403761822662\n",
      "std dev:  0.00039952624271179157 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.04730438053523614\n",
      "std dev:  0.0005676820726051027 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.017471863220496504\n",
      "std dev:  0.00035314309716256466 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.04406005491893344\n",
      "std dev:  0.0005620417965303063 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.013434263885529606\n",
      "std dev:  0.0002453162047983307 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.028903461668544895\n",
      "std dev:  0.0002950067923943189 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.07299489985221379\n",
      "std dev:  0.0003440467467849531 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.10508966944090746\n",
      "std dev:  0.0003815748601402873 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.017982048960057898\n",
      "std dev:  0.0002146836831006106 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.04141130705771962\n",
      "std dev:  0.000291273178846397 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers,trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,  0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKcZ8Q6FDBmk",
    "outputId": "18f92f6f-9eaf-4b8f-9975-86a7b71f2e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.010856780765518857\n",
      "std dev:  0.0002143349992796822 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.023666081316658216\n",
      "std dev:  0.0002208379684095288 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.023593425738424025\n",
      "std dev:  0.0004664373343340237 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.06022453044068805\n",
      "std dev:  0.0007014522401879162 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.022195567513382832\n",
      "std dev:  0.0005118490272915323 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.055615030758002934\n",
      "std dev:  0.0008033856985673512 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.019875430351089074\n",
      "std dev:  0.0002539837040937109 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.04091265720129626\n",
      "std dev:  0.0003111015335605286 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.09134288133299963\n",
      "std dev:  0.0004169327346364863 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.13151587213157248\n",
      "std dev:  0.0004754809342759335 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.024078557879602218\n",
      "std dev:  0.000254886782469845 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.05436340634087665\n",
      "std dev:  0.0003312913280755342 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, \n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,  0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiWVG7PUDDI_",
    "outputId": "1016a9c3-2997-4db4-9685-d91c3ff3537c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.014471435268682485\n",
      "std dev:  0.00023964948807985288 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.030167382295348297\n",
      "std dev:  0.0002630736974983497 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.029089770269602223\n",
      "std dev:  0.0008125347957496594 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.07323088683973648\n",
      "std dev:  0.001150157790067479 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.027332536225011205\n",
      "std dev:  0.0005776531658576562 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.06777150928551842\n",
      "std dev:  0.000948408551831532 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.027633290309810616\n",
      "std dev:  0.00033284828023189136 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.05506685468869766\n",
      "std dev:  0.00042540273087192333 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.10958970035609647\n",
      "std dev:  0.00037423870032710527 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.15777353263576888\n",
      "std dev:  0.0004595336870393098 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.031221789196753735\n",
      "std dev:  0.00030938798024647063 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.06883672081476923\n",
      "std dev:  0.00037633283857814784 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, \n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,  0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3RJCVgMDFBy",
    "outputId": "758174ce-3e1a-47da-bab0-c57a21bbc250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.01920822691808742\n",
      "std dev:  0.00030700171251544584 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.03804712766630092\n",
      "std dev:  0.0003496495123894197 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.03486987467097677\n",
      "std dev:  0.0008699133990928075 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.08656628008818239\n",
      "std dev:  0.0010693293465605623 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.033654557926358825\n",
      "std dev:  0.0010126936294687647 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.08118075804990427\n",
      "std dev:  0.0010917580993370403 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.03664185591859529\n",
      "std dev:  0.00043201942367167653 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.07143968734369308\n",
      "std dev:  0.0005049644581674387 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.1279436110528874\n",
      "std dev:  0.00037648446529890055 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.1841839174096332\n",
      "std dev:  0.00043638725873354093 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.0392285156570936\n",
      "std dev:  0.0002887723851679368 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.08478386686004705\n",
      "std dev:  0.00034945248705753107 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,  0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUmHatKjGf8Z",
    "outputId": "00dc2fee-201a-49ea-aafc-bdf4e5d96c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.0256683048771748\n",
      "std dev:  0.0004286202729592256 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.04800154547895125\n",
      "std dev:  0.0004655965585077817 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.04082072170479564\n",
      "std dev:  0.000915695323067914 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.09931039162797321\n",
      "std dev:  0.001349347109979115 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.04062457746456835\n",
      "std dev:  0.0012164010754654154 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.09503547516277991\n",
      "std dev:  0.00183662822999159 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.04687043942857138\n",
      "std dev:  0.00029404660805438637 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.0896092446066127\n",
      "std dev:  0.0003077919152917675 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.14604049939271507\n",
      "std dev:  0.0002615985067053673 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.21025262969386302\n",
      "std dev:  0.00028316593705038373 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.04864537873310547\n",
      "std dev:  0.0003606378032230164 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.10310130662754258\n",
      "std dev:  0.000405460777463862 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, \n",
    "         miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,  0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_eval_v2(X, X_minmax, scalers, test_minmax_scalers, cnn_vae, lstm_vae, batch_size, miss_forest_imputer, train_feature_means, \n",
    "             knn_imputers, lstm_ae_model, mask_prop):\n",
    "    # Creating a random mask for evaluation of imputation \n",
    "\n",
    "    iter = 30\n",
    "    cnn_vae_mse_list = []\n",
    "    cnn_vae_mae_list = []\n",
    "\n",
    "    lstm_vae_mse_list = []\n",
    "    lstm_vae_mae_list = []\n",
    "    \n",
    "    mf_mse_list = []\n",
    "    mf_mae_list = []\n",
    "    \n",
    "    mean_mse_list = []\n",
    "    mean_mae_list = []\n",
    "    \n",
    "    fill_mse_list = []\n",
    "    fill_mae_list = []\n",
    "    \n",
    "    dynimp_mse_list = []\n",
    "    dynimp_mae_list = []\n",
    "    \n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    rand_X_imputed = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "    mask_X_imputed = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "    rand_mask = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "\n",
    "    rand_mask_time = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "    rand_mask_features = np.empty([X.shape[0], X.shape[1], X.shape[2]])\n",
    "\n",
    "    for j in range(iter):\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            # Inducing randomness across channels\n",
    "            for k in range(rand_X_imputed.shape[1]):\n",
    "                rand_mask_1d = np.random.choice([0,1], X.shape[2], replace=True, p=[mask_prop, 1-mask_prop])\n",
    "                rand_mask_time[i,k] = rand_mask_1d\n",
    "                \n",
    "            # Inducing randomness across features            \n",
    "            for l in range(rand_X_imputed.shape[2]):   \n",
    "                rand_mask_1d = np.random.choice([0,1], X.shape[1], replace=True, p=[mask_prop, 1-mask_prop])\n",
    "                rand_mask_features[i,:,l] = rand_mask_1d\n",
    "                \n",
    "        rand_mask = np.logical_or(rand_mask_time, rand_mask_features)  \n",
    "        \n",
    "        rand_mask = np.where(np.isnan(X), 0, rand_mask)\n",
    "\n",
    "\n",
    "        cnn_vae_mse = 0\n",
    "        cnn_vae_mae = 0\n",
    "\n",
    "        lstm_vae_mse = 0\n",
    "        lstm_vae_mae = 0\n",
    "        \n",
    "        mf_mse = 0\n",
    "        mf_mae = 0\n",
    "        \n",
    "        mean_mse = 0\n",
    "        mean_mae = 0\n",
    "        \n",
    "        fill_mse = 0\n",
    "        fill_mae = 0\n",
    "        \n",
    "        dynimp_mse = 0\n",
    "        dynimp_mae = 0\n",
    "\n",
    "        # Actual mask of observations for comparison\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        ############################################ \n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, 0, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        \n",
    "        # Performing and evaluating cnn_vae imputation\n",
    "        cnn_vae_imputated_data = cnn_vae.predict([rand_X_imputed, rand_mask], batch_size=batch_size)\n",
    "\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        cnn_vae_imputated_data = np.where(rand_mask==0, cnn_vae_imputated_data, 0)\n",
    "        cnn_vae_imputated_data = np.where(np.isnan(X), 0, cnn_vae_imputated_data)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            cnn_vae_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], cnn_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "            cnn_vae_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], cnn_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "        cnn_vae_mse_list.append(cnn_vae_mse / n_samples)\n",
    "        cnn_vae_mae_list.append(cnn_vae_mae / n_samples)\n",
    "\n",
    "        ############################################ \n",
    "        \n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "        # lstm_vae imputation\n",
    "        rand_X_imputed = np.where(rand_mask==0, 0, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        \n",
    "        # Performing and evaluating lstm_vae imputation\n",
    "        lstm_vae_imputated_data = lstm_vae.predict([rand_X_imputed, rand_mask], batch_size=batch_size)\n",
    "\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        lstm_vae_imputated_data = np.where(rand_mask==0, lstm_vae_imputated_data, 0)\n",
    "        lstm_vae_imputated_data = np.where(np.isnan(X), 0, lstm_vae_imputated_data)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            lstm_vae_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], lstm_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "            lstm_vae_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], lstm_vae_imputated_data[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "        lstm_vae_mse_list.append(lstm_vae_mse / n_samples)\n",
    "        lstm_vae_mae_list.append(lstm_vae_mae / n_samples)\n",
    "        \n",
    "        ############################################ \n",
    "        \n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        \n",
    "        # Performing and evaluating missforest imputation\n",
    "        \n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        # imputing on the random mask data\n",
    "        rand_X_imputed_2d = np.reshape(rand_X_imputed, (rand_X_imputed.shape[0]*rand_X_imputed.shape[1], rand_X_imputed.shape[2]))\n",
    "\n",
    "        mf_X_imputed_2d = miss_forest_imputer.transform(rand_X_imputed_2d)\n",
    "        mf_X_imputed = np.reshape(mf_X_imputed_2d, (rand_X_imputed.shape[0], rand_X_imputed.shape[1], rand_X_imputed.shape[2]))\n",
    "\n",
    "        #print(rand_X_test_imputed)\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        mf_X_imputed = np.where(rand_mask==0, mf_X_imputed, 0)\n",
    "        mf_X_imputed = np.where(np.isnan(X), 0, mf_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            mf_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], mf_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            mf_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], mf_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        mf_mse_list.append(mf_mse / n_samples)\n",
    "        mf_mae_list.append(mf_mae / n_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################ \n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        \n",
    "        # Performing and evaluating mean imputation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        mean_X_imputed = np.where(np.isnan(rand_X_imputed), train_feature_means,  rand_X_imputed)\n",
    "        \n",
    "        \n",
    "        mean_X_imputed = np.where(rand_mask==0, mean_X_imputed, 0)\n",
    "        mean_X_imputed = np.where(np.isnan(X), 0, mean_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            mean_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], mean_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            mean_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], mean_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        mean_mse_list.append(mean_mse / n_samples)\n",
    "        mean_mae_list.append(mean_mae / n_samples)\n",
    "\n",
    "        ############################################ \n",
    "         # Actual mask of observations for comparison\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        # imputing on the random mask data\n",
    "        \n",
    "        for k in range(rand_X_imputed.shape[0]):\n",
    "            intermediate_df = pd.DataFrame(rand_X_imputed[k,:,:])\n",
    "            intermediate_df = intermediate_df.ffill()\n",
    "            intermediate_df = intermediate_df.bfill()\n",
    "            intermediate_array = intermediate_df.to_numpy()\n",
    "            rand_X_imputed[k,:,:] = intermediate_array\n",
    "        \n",
    "        forward_X_imputed = np.where(np.isnan(rand_X_imputed), train_feature_means,  rand_X_imputed)\n",
    "\n",
    "        #print(rand_X_test_imputed)\n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        forward_X_imputed = np.where(rand_mask==0, forward_X_imputed, 0)\n",
    "        forward_X_imputed = np.where(np.isnan(X), 0, forward_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            fill_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], forward_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            fill_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], forward_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        fill_mse_list.append(fill_mse / n_samples)\n",
    "        fill_mae_list.append(fill_mae / n_samples)\n",
    "\n",
    "        ############################################ \n",
    "         # Actual mask of observations for comparison\n",
    "        mask_X_imputed = np.where(np.isnan(X), 0, X)\n",
    "        mask_X = np.where(np.isnan(X), 0, 1)\n",
    "        # Random mask for evaluation\n",
    "        rand_X_imputed = np.where(rand_mask==0, np.nan, mask_X_imputed)\n",
    "        #print(rand_X_test_imputed[i])\n",
    "\n",
    "        #print(mask_X_test_imputed[i].shape)\n",
    "\n",
    "        # imputing on the random mask data\n",
    "        \n",
    "        rand_X_imputed_knn = np.empty([rand_X_imputed.shape[0], rand_X_imputed.shape[1], rand_X_imputed.shape[2]])\n",
    "        rand_X_imputed_transposed = rand_X_imputed.transpose((1, 0, 2))\n",
    "\n",
    "        for i in range(rand_X_imputed_transposed.shape[0]):\n",
    "\n",
    "            cur_imputer = knn_imputers[i]\n",
    "\n",
    "            rand_X_imputed_transposed[i] = cur_imputer.transform(rand_X_imputed_transposed[i])\n",
    "\n",
    "        rand_X_imputed_knn = rand_X_imputed_transposed.transpose((1, 0, 2))\n",
    "\n",
    "        dynimp_X_imputed = lstm_ae_model.predict(rand_X_imputed_knn, batch_size=batch_size)\n",
    "\n",
    "        #print(rand_X_test_imputed)\n",
    "        \n",
    "\n",
    "        # Only considering observations where actual was randomly masked out\n",
    "        dynimp_X_imputed = np.where(rand_mask==0, dynimp_X_imputed, 0)\n",
    "        dynimp_X_imputed = np.where(np.isnan(X_minmax), 0, dynimp_X_imputed)\n",
    "\n",
    "        mask_X_imputed = np.where(rand_mask==0, mask_X_imputed, 0)\n",
    "        mask_X_imputed = np.where(np.isnan(X_minmax), 0, mask_X_imputed)\n",
    "\n",
    "        rand_error_mask = np.where(rand_mask==0, 1, 0)\n",
    "        rand_error_mask = np.where(np.isnan(X_minmax), 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rand_X_imputed.shape[0]):\n",
    "            dynimp_mse += np.sum(np.square(np.subtract(mask_X_imputed[i], dynimp_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "            dynimp_mae += np.sum(np.absolute(np.subtract(mask_X_imputed[i], dynimp_X_imputed[i]))) / np.sum(rand_error_mask[i])\n",
    "\n",
    "            \n",
    "        dynimp_mse_list.append(dynimp_mse / n_samples)\n",
    "        dynimp_mae_list.append(dynimp_mae / n_samples)\n",
    "\n",
    "    print(\"fill mse:\")\n",
    "    print(\"mean: \", np.mean(fill_mse_list))\n",
    "    print(\"std dev: \",np.std(fill_mse_list), \"\\n\")\n",
    "\n",
    "\n",
    "    print(\"fill_mae:\")\n",
    "    print(\"mean: \", np.mean(fill_mae_list))\n",
    "    print(\"std dev: \",np.std(fill_mae_list), \"\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    print(\"cnn-vae imputation mse:\")\n",
    "    print(\"mean: \", np.mean(cnn_vae_mse_list))\n",
    "    print(\"std dev: \", np.std(cnn_vae_mse_list), \"\\n\")\n",
    "\n",
    "    print(\"cnn-vae imputation mae:\")\n",
    "    print(\"mean: \", np.mean(cnn_vae_mae_list))\n",
    "    print(\"std dev: \", np.std(cnn_vae_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"lstm-vae imputation mse:\")\n",
    "    print(\"mean: \", np.mean(lstm_vae_mse_list))\n",
    "    print(\"std dev: \", np.std(lstm_vae_mse_list), \"\\n\")\n",
    "\n",
    "    print(\"lstm-vae imputation mae:\")\n",
    "    print(\"mean: \", np.mean(lstm_vae_mae_list))\n",
    "    print(\"std dev: \", np.std(lstm_vae_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "    print(\"missforest imputation mse:\")\n",
    "    print(\"mean: \", np.mean(mf_mse_list))\n",
    "    print(\"std dev: \", np.std(mf_mse_list), \"\\n\")\n",
    "    \n",
    "    print(\"missforest imputation mae:\")\n",
    "    print(\"mean: \", np.mean(mf_mae_list))\n",
    "    print(\"std dev: \", np.std(mf_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"mean imputation mse:\")\n",
    "    print(\"mean: \", np.mean(mean_mse_list))\n",
    "    print(\"std dev: \", np.std(mean_mse_list), \"\\n\")\n",
    "    \n",
    "    print(\"mean imputation mae:\")\n",
    "    print(\"mean: \", np.mean(mean_mae_list))\n",
    "    print(\"std dev: \", np.std(mean_mae_list), \"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"dynimp imputation mse:\")\n",
    "    print(\"mean: \", np.mean(dynimp_mse_list))\n",
    "    print(\"std dev: \", np.std(dynimp_mse_list), \"\\n\")\n",
    "    \n",
    "    print(\"dynimp imputation mae:\")\n",
    "    print(\"mean: \", np.mean(dynimp_mae_list))\n",
    "    print(\"std dev: \", np.std(dynimp_mae_list), \"\\n\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.001637932969393205\n",
      "std dev:  6.095181654318798e-05 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.0040509205815778574\n",
      "std dev:  7.38071648493343e-05 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.004331223697644462\n",
      "std dev:  8.98482177909058e-05 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.011531009501192084\n",
      "std dev:  0.00011985266086469648 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.004250037347390458\n",
      "std dev:  0.0001262096586089028 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.010884043494927297\n",
      "std dev:  0.00019099873114046453 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.0018829519623225286\n",
      "std dev:  6.699821101341826e-05 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.005046108297050209\n",
      "std dev:  9.505882135290551e-05 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.01828557847206651\n",
      "std dev:  0.00022833955947987493 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.02630712931338543\n",
      "std dev:  0.00023111394168670124 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.004002966049396392\n",
      "std dev:  6.268800869466319e-05 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.009468509670973936\n",
      "std dev:  9.278809734200472e-05 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, \n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,\n",
    "            np.sqrt(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.0034592107486955165\n",
      "std dev:  7.119869540195453e-05 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.008343584235803265\n",
      "std dev:  9.552106047163763e-05 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.008770214327710116\n",
      "std dev:  0.00016812661543178043 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.023248394001012248\n",
      "std dev:  0.0002702089370196934 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.00846572061907902\n",
      "std dev:  0.00014748125286494416 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.021714417181368066\n",
      "std dev:  0.0002727352762477172 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.004629281577246625\n",
      "std dev:  0.0001425196598988613 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.011316332013090312\n",
      "std dev:  0.0001685213541283626 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.03641504345699072\n",
      "std dev:  0.0003709353233745481 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.05245111394680291\n",
      "std dev:  0.0004079410802158715 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.008217343266940867\n",
      "std dev:  0.00010274608297661649 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.019343903831661124\n",
      "std dev:  0.0001447140802878865 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,\n",
    "            np.sqrt(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "fill mse:\n",
      "mean:  0.005499281739638294\n",
      "std dev:  0.00013734198463631841 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.012950267440834629\n",
      "std dev:  0.00017723315828634077 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.01345361203067185\n",
      "std dev:  0.0002372431621384761 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.03528844126148272\n",
      "std dev:  0.0004105826668942746 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.01287509936806689\n",
      "std dev:  0.00025410090598299836 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.032807822261202255\n",
      "std dev:  0.0004092015577392953 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.008506715711860353\n",
      "std dev:  0.0001762800775268279 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.019302511672574144\n",
      "std dev:  0.00023333462948343036 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.0549419286971385\n",
      "std dev:  0.0003488921204221834 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.07906744220367583\n",
      "std dev:  0.0003989883732421666 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.012859588344768103\n",
      "std dev:  0.00016670025499768306 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.02999071412259419\n",
      "std dev:  0.00022320140630557037 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,\n",
    "            np.sqrt(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.007970071433116007\n",
      "std dev:  0.00014410930359812782 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.018036836510158805\n",
      "std dev:  0.0001853494557322799 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.018216373594758764\n",
      "std dev:  0.00036332181044800347 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.04742996857078483\n",
      "std dev:  0.0006034679208624073 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.017257385433836127\n",
      "std dev:  0.0003110793970587822 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.043967312714112174\n",
      "std dev:  0.00046863099686480787 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.013521147765997697\n",
      "std dev:  0.0002678546139721837 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.029026543093998648\n",
      "std dev:  0.00031420520460907054 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.07300380296085371\n",
      "std dev:  0.0002983274721354081 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.10510548097569569\n",
      "std dev:  0.00036585517912919244 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.017998587677553652\n",
      "std dev:  0.00017266865303438588 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.041427770639930174\n",
      "std dev:  0.0002500339584768796 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, \n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,\n",
    "            np.sqrt(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "fill mse:\n",
      "mean:  0.010872312183773263\n",
      "std dev:  0.00016013688374946213 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.023677233068715928\n",
      "std dev:  0.00020502495813456287 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.02343091988670491\n",
      "std dev:  0.0004891642952451823 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.060122783398446136\n",
      "std dev:  0.0007498919712745242 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.022075909555982813\n",
      "std dev:  0.0004550672735194615 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.055559511486526776\n",
      "std dev:  0.000684835035624436 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.019800582838691804\n",
      "std dev:  0.00030433168451616177 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.0408225690756204\n",
      "std dev:  0.0003693973871592346 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.0913032621844346\n",
      "std dev:  0.0003484104548443187 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.13145619358300228\n",
      "std dev:  0.0003925470300806048 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.02402283884234648\n",
      "std dev:  0.00023437787133197044 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.05427196208012652\n",
      "std dev:  0.0003070807313251406 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,\n",
    "            np.sqrt(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 1ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.014431353783102086\n",
      "std dev:  0.00029207858092645644 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.030116962740858212\n",
      "std dev:  0.00033219206423249365 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.029110403491203654\n",
      "std dev:  0.0007016049803185152 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.0731215477910521\n",
      "std dev:  0.0010892138597532047 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.02739795358502682\n",
      "std dev:  0.0007337297058475688 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.0677264064045948\n",
      "std dev:  0.001131650902704708 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.02755276123900637\n",
      "std dev:  0.00023545066266453759 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.05500697256849934\n",
      "std dev:  0.0003021987548527719 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.10956571770446578\n",
      "std dev:  0.0003238916236549443 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.1577335730916835\n",
      "std dev:  0.0003523647309442401 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.031171625323942718\n",
      "std dev:  0.00032099367777373535 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.06877642044678166\n",
      "std dev:  0.0004025581725546002 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1, \n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model,\n",
    "            np.sqrt(0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.01909970600464502\n",
      "std dev:  0.0004092257301434154 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.03791002673321606\n",
      "std dev:  0.00044491801741111426 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.03474217967518454\n",
      "std dev:  0.0008861662218164933 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.08626668689892347\n",
      "std dev:  0.0012620592884066604 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.0332323347013412\n",
      "std dev:  0.0010653542413965982 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.0805011521780667\n",
      "std dev:  0.0014582022706507746 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.036573084164469666\n",
      "std dev:  0.0003523377229909688 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.07133992477579687\n",
      "std dev:  0.0004443499259233597 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.12793699018775673\n",
      "std dev:  0.00043587496647404884 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.1841543436532977\n",
      "std dev:  0.0004558837806172205 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.03933556505326266\n",
      "std dev:  0.00023948655677965202 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.08490458250535513\n",
      "std dev:  0.0002540661560753558 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model, \n",
    "            np.sqrt(0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 11ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "101/101 [==============================] - 0s 2ms/step\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "101/101 [==============================] - 1s 10ms/step\n",
      "fill mse:\n",
      "mean:  0.02577931990933397\n",
      "std dev:  0.0004357142856300176 \n",
      "\n",
      "fill_mae:\n",
      "mean:  0.048036477191824155\n",
      "std dev:  0.0005057764750411768 \n",
      "\n",
      "cnn-vae imputation mse:\n",
      "mean:  0.041265941076449944\n",
      "std dev:  0.001121689536996606 \n",
      "\n",
      "cnn-vae imputation mae:\n",
      "mean:  0.1000241025818102\n",
      "std dev:  0.0014236744121391637 \n",
      "\n",
      "\n",
      "lstm-vae imputation mse:\n",
      "mean:  0.040953848972137295\n",
      "std dev:  0.0013175363029332922 \n",
      "\n",
      "lstm-vae imputation mae:\n",
      "mean:  0.09560311127447216\n",
      "std dev:  0.0018894166123267556 \n",
      "\n",
      "\n",
      "missforest imputation mse:\n",
      "mean:  0.04703245796165164\n",
      "std dev:  0.0003151955751771711 \n",
      "\n",
      "missforest imputation mae:\n",
      "mean:  0.08981798635027177\n",
      "std dev:  0.0003597420783978481 \n",
      "\n",
      "\n",
      "mean imputation mse:\n",
      "mean:  0.14607984432619994\n",
      "std dev:  0.00022591981891263303 \n",
      "\n",
      "mean imputation mae:\n",
      "mean:  0.21027464957260766\n",
      "std dev:  0.00028262060483831634 \n",
      "\n",
      "\n",
      "dynimp imputation mse:\n",
      "mean:  0.048629148061422066\n",
      "std dev:  0.00031178584282126777 \n",
      "\n",
      "dynimp imputation mae:\n",
      "mean:  0.1031623073722067\n",
      "std dev:  0.0003231103468515848 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_eval_v2(X_test_minmax, X_test_minmax, scalers, train_minmax_scalers, trained_cnn_vae_model, trained_lstm_vae_model, 1,\n",
    "            miss_forest_imputer, train_feature_means, knn_imputers, lstm_ae_model, \n",
    "            np.sqrt(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_knn = np.empty([X_test_minmax.shape[0], X_test_minmax.shape[1], X_test_minmax.shape[2]])\n",
    "\n",
    "X_test_transposed = X_test_minmax.transpose((1, 0, 2))\n",
    "\n",
    "for i in range(X_test_transposed.shape[0]):\n",
    "    \n",
    "    cur_imputer = knn_imputers[i]\n",
    "    \n",
    "    X_test_transposed[i] = cur_imputer.transform(X_test_transposed[i])\n",
    "    \n",
    "        \n",
    "X_test_knn = X_test_transposed.transpose((1, 0, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ae_reconstruc_train = lstm_ae_model.predict([X_train_noise, processed_X_train_minmax, train_mask], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 0\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(cnn_reconstruc_train[a][:,b], label='cnn reconstruction', c='red')\n",
    "plt.plot(lstm_reconstruc_train[a][:,b], label='lstm reconstruction', c='green')\n",
    "plt.plot(lstm_ae_reconstruc_train[a][:,b], label='dynimp reconstruction', c='purple')\n",
    "plt.plot(processed_X_train_minmax[a][:,b], c='blue', label='original', alpha=0.6)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ae_reconstruc_test = lstm_ae_model.predict([X_test_knn, processed_X_test_minmax, test_mask], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "ZZ7l1FCGkn69",
    "outputId": "d78d56b3-105f-4457-aa18-300f145e96c5"
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 0\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(cnn_reconstruc_test[a][:,b], label='cnn reconstruction', c='red')\n",
    "plt.plot(lstm_reconstruc_test[a][:,b], label='lstm reconstruction', c='green')\n",
    "plt.plot(lstm_ae_reconstruc_test[a][:,b], label='dynimp reconstruction', c='purple')\n",
    "plt.plot(processed_X_test_minmax[a][:,b], c='blue', label='original', alpha=0.6)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputed_vae_data(X_train, X_test, reconstruc_train):\n",
    "  \n",
    "    X_train_imputed = np.empty([X_train.shape[0], X_train.shape[1], X_train.shape[2]])\n",
    "    X_test_imputed = np.empty([X_test.shape[0], X_test.shape[1], X_test.shape[2]])\n",
    "\n",
    "\n",
    "    # Impute original with reconstruction\n",
    "\n",
    "    for i in range(X_train.shape[0]):\n",
    "        for j in range(X_train.shape[1]):\n",
    "            for k in range(X_train.shape[2]):\n",
    "                if np.isnan(X_train[i,j,k]):\n",
    "                    X_train_imputed[i,j,k] = reconstruc_train[i,j,k]\n",
    "                else:\n",
    "                    X_train_imputed[i,j,k] = X_train[i,j,k]\n",
    "\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        for j in range(X_test.shape[1]):\n",
    "            for k in range(X_test.shape[2]):\n",
    "                if np.isnan(X_test[i,j,k]):\n",
    "                    X_test_imputed[i,j,k] = reconstruc_test[i,j,k]\n",
    "                else:\n",
    "                    X_test_imputed[i,j,k] = X_test[i,j,k]\n",
    "\n",
    "    return X_train_imputed, X_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45GgUyux-p9z"
   },
   "outputs": [],
   "source": [
    "cnn_X_train_imputed, cnn_X_test_imputed = imputed_vae_data(X_train, X_test, cnn_reconstruc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_X_train_imputed, lstm_X_test_imputed = imputed_vae_data(X_train, X_test, lstm_reconstruc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE44FC8KFaL2"
   },
   "outputs": [],
   "source": [
    "def readm_preprocessing(X_train_imputed, X_test_imputed, y_train, y_test):\n",
    "    readm_X_train = np.empty([X_train_imputed.shape[0], X_train_imputed.shape[1], X_train_imputed.shape[2]])\n",
    "    readm_X_test = np.empty([X_test_imputed.shape[0], X_test_imputed.shape[1], X_test_imputed.shape[2]])\n",
    "\n",
    "    for i in range(X_train_imputed.shape[0]):\n",
    "        for j in range(X_train_imputed.shape[1]):\n",
    "            for k in range(X_train_imputed.shape[2]):\n",
    "                readm_X_train[i,j,k] = X_train_imputed[i,j,k]\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(X_test_imputed.shape[0]):\n",
    "        for j in range(X_test_imputed.shape[1]):\n",
    "            for k in range(X_test_imputed.shape[2]):\n",
    "                readm_X_test[i,j,k] = X_test_imputed[i,j,k]\n",
    "\n",
    "\n",
    "\n",
    "    readm_y_train = y_train['readmission']\n",
    "    readm_y_test = y_test['readmission']\n",
    "\n",
    "\n",
    "    rm_idx_train = []\n",
    "    rm_idx_test = []\n",
    "\n",
    "\n",
    "    for i in range(readm_X_train.shape[0]):\n",
    "        if np.isnan(y_train['readmission'].values[i]) or y_train['mortality'].values[i] == 1:\n",
    "            rm_idx_train.append(i)\n",
    "\n",
    "    for i in range(readm_X_test.shape[0]):\n",
    "        if np.isnan(y_test['readmission'].values[i]) or y_train['mortality'].values[i] == 1:\n",
    "            rm_idx_test.append(i)\n",
    "\n",
    "    readm_X_train = np.delete(readm_X_train, rm_idx_train, 0)\n",
    "    readm_y_train = np.delete(np.array(readm_y_train), rm_idx_train, 0)\n",
    "\n",
    "    readm_X_test = np.delete(readm_X_test, rm_idx_test, 0)\n",
    "    readm_y_test = np.delete(np.array(readm_y_test), rm_idx_test, 0)\n",
    "\n",
    "    #print(readm_X_train.shape)\n",
    "    #print(readm_y_train.shape)\n",
    "\n",
    "    #print(readm_X_test.shape)\n",
    "    #print(readm_y_test.shape)\n",
    "\n",
    "    #print(np.where(readm_y_train == 1))\n",
    "    #print(np.where(readm_y_test == 1))  \n",
    "    return readm_X_train, readm_X_test, readm_y_train, readm_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9Nk7yYjFk_n"
   },
   "outputs": [],
   "source": [
    "def mortality_preprocessing(X_train_imputed, X_test_imputed, y_train, y_test):\n",
    "    # Processing Data for Mortality\n",
    "    mortality_X_train = np.empty([X_train_imputed.shape[0], X_train_imputed.shape[1], X_train_imputed.shape[2]])\n",
    "    mortality_X_test = np.empty([X_test_imputed.shape[0], X_test_imputed.shape[1], X_test_imputed.shape[2]])\n",
    "\n",
    "    for i in range(X_train_imputed.shape[0]):\n",
    "        for j in range(X_train_imputed.shape[1]):\n",
    "            for k in range(X_train_imputed.shape[2]):\n",
    "                mortality_X_train[i,j,k] = X_train_imputed[i,j,k]\n",
    "\n",
    "\n",
    "    for i in range(X_test_imputed.shape[0]):\n",
    "        for j in range(X_test_imputed.shape[1]):\n",
    "            for k in range(X_test_imputed.shape[2]):\n",
    "                mortality_X_test[i,j,k] = X_test_imputed[i,j,k]\n",
    "\n",
    "    mortality_y_train = y_train['mortality']\n",
    "    mortality_y_test = y_test['mortality']\n",
    "\n",
    "\n",
    "    #print(np.where(mortality_y_train == 1))\n",
    "    #print(np.where(mortality_y_test == 1))\n",
    "    return mortality_X_train, mortality_X_test, mortality_y_train, mortality_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNeU2yqMFlHo"
   },
   "outputs": [],
   "source": [
    "def los_preprocessing(X_train_imputed, X_test_imputed, y_train, y_test):\n",
    "    # Processing Data for Length of Stay\n",
    "    los_X_train = np.empty([X_train_imputed.shape[0], X_train_imputed.shape[1], X_train_imputed.shape[2]])\n",
    "    los_X_test = np.empty([X_test_imputed.shape[0], X_test_imputed.shape[1], X_test_imputed.shape[2]])\n",
    "\n",
    "    for i in range(X_train_imputed.shape[0]):\n",
    "        for j in range(X_train_imputed.shape[1]):\n",
    "            for k in range(X_train_imputed.shape[2]):\n",
    "                los_X_train[i,j,k] = X_train_imputed[i,j,k]\n",
    "\n",
    "\n",
    "    for i in range(X_test_imputed.shape[0]):\n",
    "        for j in range(X_test_imputed.shape[1]):\n",
    "            for k in range(X_test_imputed.shape[2]):\n",
    "                los_X_test[i,j,k] = X_test_imputed[i,j,k]\n",
    "\n",
    "                \n",
    "    los_y_train = y_train['length_of_stay']\n",
    "    los_y_test = y_test['length_of_stay']\n",
    "    \n",
    "    rm_idx_train = []\n",
    "    rm_idx_test = []\n",
    "\n",
    "\n",
    "    for i in range(los_X_train.shape[0]):\n",
    "        if los_y_train.values[i] < 0:\n",
    "            rm_idx_train.append(i)\n",
    "\n",
    "    for i in range(los_X_test.shape[0]):\n",
    "        if los_y_test.values[i] < 0:\n",
    "            rm_idx_test.append(i)\n",
    "\n",
    "    los_X_train = np.delete(los_X_train, rm_idx_train, 0)\n",
    "    los_y_train = np.delete(np.array(los_y_train), rm_idx_train, 0)\n",
    "\n",
    "    los_X_test = np.delete(los_X_test, rm_idx_test, 0)\n",
    "    los_y_test = np.delete(np.array(los_y_test), rm_idx_test, 0)\n",
    "    \n",
    "          \n",
    "    los_y_train = (los_y_train - np.full(len(los_y_train), np.mean(los_y_train))) / np.std(los_y_train)\n",
    "    \n",
    "    los_y_test = (los_y_test - np.full(len(los_y_test), np.mean(los_y_test))) / np.std(los_y_test)\n",
    "  \n",
    "\n",
    "    return los_X_train, los_X_test, los_y_train, los_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BmHqSFHGzPk"
   },
   "outputs": [],
   "source": [
    "# Readmission data for each method\n",
    "readm_mean_X_train, readm_mean_X_test, readm_mean_y_train, readm_mean_y_test = readm_preprocessing(X_train_mean_imputed_1, \n",
    "                                                                               X_test_mean_imputed_1, y_train, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Mortality data for each method\n",
    "mortality_mean_X_train, mortality_mean_X_test, mortality_mean_y_train, mortality_mean_y_test = mortality_preprocessing(X_train_mean_imputed_1, \n",
    "                                                                               X_test_mean_imputed_1, y_train, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Length of stay data for each method\n",
    "los_mean_X_train, los_mean_X_test, los_mean_y_train, los_mean_y_test = los_preprocessing(X_train_mean_imputed_1, \n",
    "                                                                               X_test_mean_imputed_1, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPgncEK1QFNV"
   },
   "outputs": [],
   "source": [
    "# LSTM Classification Model\n",
    "\n",
    "es = EarlyStopping(patience=20, verbose=0, min_delta=0.0001, monitor='val_auc', mode='auto', restore_best_weights=True)\n",
    "\n",
    "class_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.LSTM(64, activation='tanh', return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, activation='tanh', return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, activation='tanh'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "class_model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.AUC(curve='PR'), \n",
    "                      tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfNt7NW2QRXj"
   },
   "outputs": [],
   "source": [
    "# LSTM Regression Model\n",
    "\n",
    "es = EarlyStopping(patience=10, verbose=0, min_delta=0.0001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "\n",
    "reg_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.LSTM(64, activation='tanh', return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, activation='tanh', return_sequences=True),\n",
    "  tf.keras.layers.LSTM(64, activation='tanh'),\n",
    "  tf.keras.layers.Dense(1, activation='relu'),\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "reg_model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvef8fxg-p91"
   },
   "outputs": [],
   "source": [
    "def train_eval_pred_model(model, batch_size, epochs, X_train, X_test, y_train, y_test):\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2, callbacks=[es])\n",
    "    \n",
    "    predictions = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "    metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkxhg1QY-p91",
    "outputId": "3d8f3de0-2152-4e93-b20b-2e12399dcc1a"
   },
   "outputs": [],
   "source": [
    "mean_mort_model, mean_mort_preds = train_eval_pred_model(class_model, 2, 200, mortality_mean_X_train, mortality_mean_X_test,\n",
    "                                                          mortality_mean_y_train, mortality_mean_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3MTbmxZ-p92",
    "outputId": "2cc9ea8a-0899-48e3-cea4-bec7e593ec25"
   },
   "outputs": [],
   "source": [
    "mean_los_model, mean_los_preds  = train_eval_pred_model(reg_model, 1, 200, los_mean_X_train, los_mean_X_test, los_mean_y_train, los_mean_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7hBaf-KwOBc9",
    "outputId": "b2773897-6b07-480a-9561-02fa4079d950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7197/7197 - 135s - loss: 0.4768 - auc_2: 0.5090 - auc_3: 0.1573 - precision_1: 0.1132 - recall_1: 0.0053 - val_loss: 0.4265 - val_auc_2: 0.5144 - val_auc_3: 0.1438 - val_precision_1: 0.0833 - val_recall_1: 0.0019 - 135s/epoch - 19ms/step\n",
      "Epoch 2/200\n",
      "7197/7197 - 133s - loss: 0.4434 - auc_2: 0.5391 - auc_3: 0.1714 - precision_1: 0.1136 - recall_1: 0.0022 - val_loss: 0.4231 - val_auc_2: 0.5204 - val_auc_3: 0.1472 - val_precision_1: 0.1667 - val_recall_1: 0.0019 - 133s/epoch - 19ms/step\n",
      "Epoch 3/200\n",
      "7197/7197 - 133s - loss: 0.4403 - auc_2: 0.5453 - auc_3: 0.1752 - precision_1: 0.1481 - recall_1: 0.0018 - val_loss: 0.4211 - val_auc_2: 0.5251 - val_auc_3: 0.1495 - val_precision_1: 0.2500 - val_recall_1: 0.0019 - 133s/epoch - 18ms/step\n",
      "Epoch 4/200\n",
      "7197/7197 - 131s - loss: 0.4386 - auc_2: 0.5522 - auc_3: 0.1790 - precision_1: 0.1364 - recall_1: 0.0013 - val_loss: 0.4198 - val_auc_2: 0.5310 - val_auc_3: 0.1519 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 5/200\n",
      "7197/7197 - 131s - loss: 0.4373 - auc_2: 0.5562 - auc_3: 0.1813 - precision_1: 0.1176 - recall_1: 8.8106e-04 - val_loss: 0.4185 - val_auc_2: 0.5318 - val_auc_3: 0.1522 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 6/200\n",
      "7197/7197 - 133s - loss: 0.4364 - auc_2: 0.5606 - auc_3: 0.1838 - precision_1: 0.1000 - recall_1: 4.4053e-04 - val_loss: 0.4185 - val_auc_2: 0.5368 - val_auc_3: 0.1542 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 133s/epoch - 19ms/step\n",
      "Epoch 7/200\n",
      "7197/7197 - 133s - loss: 0.4356 - auc_2: 0.5666 - auc_3: 0.1870 - precision_1: 0.1000 - recall_1: 4.4053e-04 - val_loss: 0.4173 - val_auc_2: 0.5392 - val_auc_3: 0.1553 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 133s/epoch - 18ms/step\n",
      "Epoch 8/200\n",
      "7197/7197 - 132s - loss: 0.4349 - auc_2: 0.5686 - auc_3: 0.1886 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4170 - val_auc_2: 0.5422 - val_auc_3: 0.1566 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 132s/epoch - 18ms/step\n",
      "Epoch 9/200\n",
      "7197/7197 - 131s - loss: 0.4343 - auc_2: 0.5719 - auc_3: 0.1905 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4163 - val_auc_2: 0.5425 - val_auc_3: 0.1566 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 10/200\n",
      "7197/7197 - 130s - loss: 0.4337 - auc_2: 0.5731 - auc_3: 0.1925 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4162 - val_auc_2: 0.5446 - val_auc_3: 0.1582 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 11/200\n",
      "7197/7197 - 130s - loss: 0.4332 - auc_2: 0.5757 - auc_3: 0.1936 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4159 - val_auc_2: 0.5466 - val_auc_3: 0.1587 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 12/200\n",
      "7197/7197 - 133s - loss: 0.4328 - auc_2: 0.5795 - auc_3: 0.1964 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4152 - val_auc_2: 0.5479 - val_auc_3: 0.1596 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 133s/epoch - 18ms/step\n",
      "Epoch 13/200\n",
      "7197/7197 - 132s - loss: 0.4323 - auc_2: 0.5811 - auc_3: 0.1978 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4153 - val_auc_2: 0.5474 - val_auc_3: 0.1594 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 132s/epoch - 18ms/step\n",
      "Epoch 14/200\n",
      "7197/7197 - 131s - loss: 0.4319 - auc_2: 0.5843 - auc_3: 0.1997 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4147 - val_auc_2: 0.5511 - val_auc_3: 0.1615 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 15/200\n",
      "7197/7197 - 132s - loss: 0.4315 - auc_2: 0.5877 - auc_3: 0.2017 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4145 - val_auc_2: 0.5512 - val_auc_3: 0.1617 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 132s/epoch - 18ms/step\n",
      "Epoch 16/200\n",
      "7197/7197 - 130s - loss: 0.4311 - auc_2: 0.5870 - auc_3: 0.2021 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4147 - val_auc_2: 0.5497 - val_auc_3: 0.1617 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 17/200\n",
      "7197/7197 - 129s - loss: 0.4308 - auc_2: 0.5908 - auc_3: 0.2047 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4147 - val_auc_2: 0.5503 - val_auc_3: 0.1622 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 129s/epoch - 18ms/step\n",
      "Epoch 18/200\n",
      "7197/7197 - 134s - loss: 0.4304 - auc_2: 0.5921 - auc_3: 0.2053 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4140 - val_auc_2: 0.5535 - val_auc_3: 0.1633 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 134s/epoch - 19ms/step\n",
      "Epoch 19/200\n",
      "7197/7197 - 138s - loss: 0.4301 - auc_2: 0.5935 - auc_3: 0.2064 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4136 - val_auc_2: 0.5553 - val_auc_3: 0.1641 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 138s/epoch - 19ms/step\n",
      "Epoch 20/200\n",
      "7197/7197 - 134s - loss: 0.4298 - auc_2: 0.5950 - auc_3: 0.2074 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4138 - val_auc_2: 0.5541 - val_auc_3: 0.1644 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 134s/epoch - 19ms/step\n",
      "Epoch 21/200\n",
      "7197/7197 - 131s - loss: 0.4295 - auc_2: 0.5969 - auc_3: 0.2089 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4132 - val_auc_2: 0.5566 - val_auc_3: 0.1649 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 22/200\n",
      "7197/7197 - 131s - loss: 0.4292 - auc_2: 0.5987 - auc_3: 0.2103 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4135 - val_auc_2: 0.5553 - val_auc_3: 0.1651 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 23/200\n",
      "7197/7197 - 131s - loss: 0.4289 - auc_2: 0.5987 - auc_3: 0.2108 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4126 - val_auc_2: 0.5592 - val_auc_3: 0.1663 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 24/200\n",
      "7197/7197 - 131s - loss: 0.4287 - auc_2: 0.5998 - auc_3: 0.2120 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4125 - val_auc_2: 0.5600 - val_auc_3: 0.1668 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 25/200\n",
      "7197/7197 - 131s - loss: 0.4284 - auc_2: 0.6009 - auc_3: 0.2122 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4127 - val_auc_2: 0.5594 - val_auc_3: 0.1669 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 26/200\n",
      "7197/7197 - 131s - loss: 0.4281 - auc_2: 0.6028 - auc_3: 0.2140 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4124 - val_auc_2: 0.5608 - val_auc_3: 0.1670 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 27/200\n",
      "7197/7197 - 131s - loss: 0.4279 - auc_2: 0.6031 - auc_3: 0.2143 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.4123 - val_auc_2: 0.5618 - val_auc_3: 0.1683 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 28/200\n",
      "7197/7197 - 130s - loss: 0.4277 - auc_2: 0.6042 - auc_3: 0.2149 - precision_1: 0.5000 - recall_1: 4.4053e-04 - val_loss: 0.4123 - val_auc_2: 0.5623 - val_auc_3: 0.1685 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 29/200\n",
      "7197/7197 - 131s - loss: 0.4274 - auc_2: 0.6057 - auc_3: 0.2158 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4118 - val_auc_2: 0.5636 - val_auc_3: 0.1687 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 30/200\n",
      "7197/7197 - 130s - loss: 0.4273 - auc_2: 0.6057 - auc_3: 0.2165 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4121 - val_auc_2: 0.5641 - val_auc_3: 0.1695 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 31/200\n",
      "7197/7197 - 131s - loss: 0.4271 - auc_2: 0.6066 - auc_3: 0.2165 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4122 - val_auc_2: 0.5630 - val_auc_3: 0.1695 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 32/200\n",
      "7197/7197 - 131s - loss: 0.4268 - auc_2: 0.6085 - auc_3: 0.2175 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4126 - val_auc_2: 0.5622 - val_auc_3: 0.1698 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 33/200\n",
      "7197/7197 - 130s - loss: 0.4266 - auc_2: 0.6087 - auc_3: 0.2177 - precision_1: 0.5000 - recall_1: 4.4053e-04 - val_loss: 0.4117 - val_auc_2: 0.5654 - val_auc_3: 0.1704 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 34/200\n",
      "7197/7197 - 131s - loss: 0.4265 - auc_2: 0.6098 - auc_3: 0.2187 - precision_1: 0.5000 - recall_1: 4.4053e-04 - val_loss: 0.4125 - val_auc_2: 0.5633 - val_auc_3: 0.1707 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 35/200\n",
      "7197/7197 - 131s - loss: 0.4263 - auc_2: 0.6105 - auc_3: 0.2197 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4119 - val_auc_2: 0.5658 - val_auc_3: 0.1714 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 36/200\n",
      "7197/7197 - 132s - loss: 0.4261 - auc_2: 0.6110 - auc_3: 0.2199 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4115 - val_auc_2: 0.5660 - val_auc_3: 0.1713 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 132s/epoch - 18ms/step\n",
      "Epoch 37/200\n",
      "7197/7197 - 131s - loss: 0.4259 - auc_2: 0.6109 - auc_3: 0.2206 - precision_1: 0.5000 - recall_1: 4.4053e-04 - val_loss: 0.4120 - val_auc_2: 0.5656 - val_auc_3: 0.1715 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 38/200\n",
      "7197/7197 - 131s - loss: 0.4258 - auc_2: 0.6117 - auc_3: 0.2208 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4115 - val_auc_2: 0.5663 - val_auc_3: 0.1717 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 39/200\n",
      "7197/7197 - 131s - loss: 0.4257 - auc_2: 0.6123 - auc_3: 0.2215 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4119 - val_auc_2: 0.5656 - val_auc_3: 0.1720 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 40/200\n",
      "7197/7197 - 131s - loss: 0.4254 - auc_2: 0.6132 - auc_3: 0.2219 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4113 - val_auc_2: 0.5666 - val_auc_3: 0.1715 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 41/200\n",
      "7197/7197 - 131s - loss: 0.4253 - auc_2: 0.6144 - auc_3: 0.2225 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4115 - val_auc_2: 0.5680 - val_auc_3: 0.1726 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 42/200\n",
      "7197/7197 - 131s - loss: 0.4253 - auc_2: 0.6145 - auc_3: 0.2223 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4121 - val_auc_2: 0.5659 - val_auc_3: 0.1723 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 43/200\n",
      "7197/7197 - 131s - loss: 0.4251 - auc_2: 0.6153 - auc_3: 0.2229 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4132 - val_auc_2: 0.5647 - val_auc_3: 0.1733 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 44/200\n",
      "7197/7197 - 131s - loss: 0.4250 - auc_2: 0.6144 - auc_3: 0.2233 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4118 - val_auc_2: 0.5671 - val_auc_3: 0.1727 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 45/200\n",
      "7197/7197 - 131s - loss: 0.4248 - auc_2: 0.6153 - auc_3: 0.2240 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4113 - val_auc_2: 0.5679 - val_auc_3: 0.1725 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 46/200\n",
      "7197/7197 - 131s - loss: 0.4247 - auc_2: 0.6160 - auc_3: 0.2245 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4128 - val_auc_2: 0.5653 - val_auc_3: 0.1734 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 47/200\n",
      "7197/7197 - 131s - loss: 0.4246 - auc_2: 0.6161 - auc_3: 0.2252 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4114 - val_auc_2: 0.5678 - val_auc_3: 0.1728 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 48/200\n",
      "7197/7197 - 131s - loss: 0.4245 - auc_2: 0.6174 - auc_3: 0.2249 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4118 - val_auc_2: 0.5668 - val_auc_3: 0.1730 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 49/200\n",
      "7197/7197 - 130s - loss: 0.4243 - auc_2: 0.6189 - auc_3: 0.2255 - precision_1: 0.3333 - recall_1: 4.4053e-04 - val_loss: 0.4125 - val_auc_2: 0.5651 - val_auc_3: 0.1730 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 130s/epoch - 18ms/step\n",
      "Epoch 50/200\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "7197/7197 - 131s - loss: 0.4241 - auc_2: 0.6187 - auc_3: 0.2267 - precision_1: 0.2500 - recall_1: 4.4053e-04 - val_loss: 0.4127 - val_auc_2: 0.5656 - val_auc_3: 0.1735 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - 131s/epoch - 18ms/step\n",
      "Epoch 50: early stopping\n",
      "2253/2253 [==============================] - 23s 10ms/step - loss: 0.4108 - auc_2: 0.5810 - auc_3: 0.1776 - precision_1: 0.3333 - recall_1: 0.0031\n"
     ]
    }
   ],
   "source": [
    "# cnn-vae\n",
    "cnn_readm_model, cnn_readm_preds  = train_eval_pred_model(class_model, 2, 200, readm_cnn_X_train, readm_cnn_X_test,\n",
    "                                        readm_cnn_y_train, readm_cnn_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1PO5DwH-p92",
    "outputId": "e2874155-639f-4bb6-98c1-57c0b3fceab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10076/10076 - 193s - loss: 0.2468 - auc_2: 0.8361 - auc_3: 0.4236 - precision_1: 0.8070 - recall_1: 0.0240 - val_loss: 0.2398 - val_auc_2: 0.8499 - val_auc_3: 0.4834 - val_precision_1: 0.9333 - val_recall_1: 0.0566 - 193s/epoch - 19ms/step\n",
      "Epoch 2/200\n",
      "10076/10076 - 192s - loss: 0.2260 - auc_2: 0.8635 - auc_3: 0.4987 - precision_1: 0.7645 - recall_1: 0.1100 - val_loss: 0.2309 - val_auc_2: 0.8591 - val_auc_3: 0.5119 - val_precision_1: 0.7923 - val_recall_1: 0.2081 - 192s/epoch - 19ms/step\n",
      "Epoch 3/200\n",
      "10076/10076 - 192s - loss: 0.2174 - auc_2: 0.8733 - auc_3: 0.5251 - precision_1: 0.7657 - recall_1: 0.2419 - val_loss: 0.2254 - val_auc_2: 0.8660 - val_auc_3: 0.5272 - val_precision_1: 0.7526 - val_recall_1: 0.2889 - 192s/epoch - 19ms/step\n",
      "Epoch 4/200\n",
      "10076/10076 - 192s - loss: 0.2117 - auc_2: 0.8798 - auc_3: 0.5421 - precision_1: 0.7324 - recall_1: 0.3097 - val_loss: 0.2220 - val_auc_2: 0.8696 - val_auc_3: 0.5378 - val_precision_1: 0.7487 - val_recall_1: 0.2949 - 192s/epoch - 19ms/step\n",
      "Epoch 5/200\n",
      "10076/10076 - 192s - loss: 0.2078 - auc_2: 0.8847 - auc_3: 0.5581 - precision_1: 0.7372 - recall_1: 0.3306 - val_loss: 0.2193 - val_auc_2: 0.8731 - val_auc_3: 0.5482 - val_precision_1: 0.7342 - val_recall_1: 0.3293 - 192s/epoch - 19ms/step\n",
      "Epoch 6/200\n",
      "10076/10076 - 192s - loss: 0.2046 - auc_2: 0.8877 - auc_3: 0.5708 - precision_1: 0.7302 - recall_1: 0.3514 - val_loss: 0.2173 - val_auc_2: 0.8753 - val_auc_3: 0.5576 - val_precision_1: 0.7167 - val_recall_1: 0.3374 - 192s/epoch - 19ms/step\n",
      "Epoch 7/200\n",
      "10076/10076 - 191s - loss: 0.2021 - auc_2: 0.8901 - auc_3: 0.5819 - precision_1: 0.7308 - recall_1: 0.3624 - val_loss: 0.2153 - val_auc_2: 0.8767 - val_auc_3: 0.5649 - val_precision_1: 0.7316 - val_recall_1: 0.3414 - 191s/epoch - 19ms/step\n",
      "Epoch 8/200\n",
      "10076/10076 - 193s - loss: 0.2000 - auc_2: 0.8927 - auc_3: 0.5898 - precision_1: 0.7306 - recall_1: 0.3676 - val_loss: 0.2137 - val_auc_2: 0.8778 - val_auc_3: 0.5706 - val_precision_1: 0.7155 - val_recall_1: 0.3455 - 193s/epoch - 19ms/step\n",
      "Epoch 9/200\n",
      "10076/10076 - 192s - loss: 0.1984 - auc_2: 0.8942 - auc_3: 0.5977 - precision_1: 0.7379 - recall_1: 0.3728 - val_loss: 0.2126 - val_auc_2: 0.8784 - val_auc_3: 0.5744 - val_precision_1: 0.6935 - val_recall_1: 0.3475 - 192s/epoch - 19ms/step\n",
      "Epoch 10/200\n",
      "10076/10076 - 191s - loss: 0.1970 - auc_2: 0.8952 - auc_3: 0.6017 - precision_1: 0.7402 - recall_1: 0.3759 - val_loss: 0.2119 - val_auc_2: 0.8797 - val_auc_3: 0.5779 - val_precision_1: 0.6973 - val_recall_1: 0.3677 - 191s/epoch - 19ms/step\n",
      "Epoch 11/200\n",
      "10076/10076 - 191s - loss: 0.1959 - auc_2: 0.8962 - auc_3: 0.6070 - precision_1: 0.7421 - recall_1: 0.3796 - val_loss: 0.2111 - val_auc_2: 0.8807 - val_auc_3: 0.5804 - val_precision_1: 0.7082 - val_recall_1: 0.3677 - 191s/epoch - 19ms/step\n",
      "Epoch 12/200\n",
      "10076/10076 - 191s - loss: 0.1948 - auc_2: 0.8975 - auc_3: 0.6117 - precision_1: 0.7472 - recall_1: 0.3806 - val_loss: 0.2110 - val_auc_2: 0.8814 - val_auc_3: 0.5820 - val_precision_1: 0.6985 - val_recall_1: 0.3838 - 191s/epoch - 19ms/step\n",
      "Epoch 13/200\n",
      "10076/10076 - 191s - loss: 0.1941 - auc_2: 0.8979 - auc_3: 0.6147 - precision_1: 0.7390 - recall_1: 0.3853 - val_loss: 0.2102 - val_auc_2: 0.8827 - val_auc_3: 0.5854 - val_precision_1: 0.7011 - val_recall_1: 0.3838 - 191s/epoch - 19ms/step\n",
      "Epoch 14/200\n",
      "10076/10076 - 191s - loss: 0.1934 - auc_2: 0.8988 - auc_3: 0.6174 - precision_1: 0.7431 - recall_1: 0.3905 - val_loss: 0.2094 - val_auc_2: 0.8821 - val_auc_3: 0.5873 - val_precision_1: 0.7298 - val_recall_1: 0.3657 - 191s/epoch - 19ms/step\n",
      "Epoch 15/200\n",
      "10076/10076 - 192s - loss: 0.1927 - auc_2: 0.8992 - auc_3: 0.6200 - precision_1: 0.7525 - recall_1: 0.3884 - val_loss: 0.2091 - val_auc_2: 0.8834 - val_auc_3: 0.5888 - val_precision_1: 0.7030 - val_recall_1: 0.3778 - 192s/epoch - 19ms/step\n",
      "Epoch 16/200\n",
      "10076/10076 - 191s - loss: 0.1922 - auc_2: 0.8998 - auc_3: 0.6218 - precision_1: 0.7483 - recall_1: 0.3921 - val_loss: 0.2088 - val_auc_2: 0.8841 - val_auc_3: 0.5902 - val_precision_1: 0.7137 - val_recall_1: 0.3778 - 191s/epoch - 19ms/step\n",
      "Epoch 17/200\n",
      "10076/10076 - 191s - loss: 0.1916 - auc_2: 0.9005 - auc_3: 0.6231 - precision_1: 0.7458 - recall_1: 0.3962 - val_loss: 0.2081 - val_auc_2: 0.8846 - val_auc_3: 0.5930 - val_precision_1: 0.7148 - val_recall_1: 0.3798 - 191s/epoch - 19ms/step\n",
      "Epoch 18/200\n",
      "10076/10076 - 191s - loss: 0.1910 - auc_2: 0.9010 - auc_3: 0.6264 - precision_1: 0.7520 - recall_1: 0.3999 - val_loss: 0.2078 - val_auc_2: 0.8852 - val_auc_3: 0.5931 - val_precision_1: 0.7266 - val_recall_1: 0.3758 - 191s/epoch - 19ms/step\n",
      "Epoch 19/200\n",
      "10076/10076 - 192s - loss: 0.1907 - auc_2: 0.9011 - auc_3: 0.6273 - precision_1: 0.7507 - recall_1: 0.3973 - val_loss: 0.2077 - val_auc_2: 0.8851 - val_auc_3: 0.5936 - val_precision_1: 0.7121 - val_recall_1: 0.3798 - 192s/epoch - 19ms/step\n",
      "Epoch 20/200\n",
      "10076/10076 - 190s - loss: 0.1901 - auc_2: 0.9022 - auc_3: 0.6288 - precision_1: 0.7524 - recall_1: 0.4009 - val_loss: 0.2078 - val_auc_2: 0.8848 - val_auc_3: 0.5944 - val_precision_1: 0.6974 - val_recall_1: 0.3818 - 190s/epoch - 19ms/step\n",
      "Epoch 21/200\n",
      "10076/10076 - 190s - loss: 0.1898 - auc_2: 0.9024 - auc_3: 0.6302 - precision_1: 0.7531 - recall_1: 0.4056 - val_loss: 0.2075 - val_auc_2: 0.8861 - val_auc_3: 0.5965 - val_precision_1: 0.7000 - val_recall_1: 0.3960 - 190s/epoch - 19ms/step\n",
      "Epoch 22/200\n",
      "10076/10076 - 191s - loss: 0.1895 - auc_2: 0.9025 - auc_3: 0.6320 - precision_1: 0.7519 - recall_1: 0.4093 - val_loss: 0.2068 - val_auc_2: 0.8869 - val_auc_3: 0.5988 - val_precision_1: 0.7228 - val_recall_1: 0.3899 - 191s/epoch - 19ms/step\n",
      "Epoch 23/200\n",
      "10076/10076 - 191s - loss: 0.1892 - auc_2: 0.9028 - auc_3: 0.6327 - precision_1: 0.7472 - recall_1: 0.4145 - val_loss: 0.2065 - val_auc_2: 0.8860 - val_auc_3: 0.5990 - val_precision_1: 0.7300 - val_recall_1: 0.3879 - 191s/epoch - 19ms/step\n",
      "Epoch 24/200\n",
      "10076/10076 - 191s - loss: 0.1888 - auc_2: 0.9032 - auc_3: 0.6351 - precision_1: 0.7538 - recall_1: 0.4181 - val_loss: 0.2063 - val_auc_2: 0.8869 - val_auc_3: 0.6010 - val_precision_1: 0.7331 - val_recall_1: 0.3939 - 191s/epoch - 19ms/step\n",
      "Epoch 25/200\n",
      "10076/10076 - 191s - loss: 0.1885 - auc_2: 0.9035 - auc_3: 0.6358 - precision_1: 0.7555 - recall_1: 0.4155 - val_loss: 0.2063 - val_auc_2: 0.8873 - val_auc_3: 0.5990 - val_precision_1: 0.6958 - val_recall_1: 0.4020 - 191s/epoch - 19ms/step\n",
      "Epoch 26/200\n",
      "10076/10076 - 191s - loss: 0.1881 - auc_2: 0.9038 - auc_3: 0.6370 - precision_1: 0.7509 - recall_1: 0.4228 - val_loss: 0.2056 - val_auc_2: 0.8868 - val_auc_3: 0.6011 - val_precision_1: 0.7323 - val_recall_1: 0.3980 - 191s/epoch - 19ms/step\n",
      "Epoch 27/200\n",
      "10076/10076 - 190s - loss: 0.1879 - auc_2: 0.9041 - auc_3: 0.6369 - precision_1: 0.7587 - recall_1: 0.4228 - val_loss: 0.2053 - val_auc_2: 0.8877 - val_auc_3: 0.6026 - val_precision_1: 0.7263 - val_recall_1: 0.4020 - 190s/epoch - 19ms/step\n",
      "Epoch 28/200\n",
      "10076/10076 - 192s - loss: 0.1876 - auc_2: 0.9041 - auc_3: 0.6396 - precision_1: 0.7565 - recall_1: 0.4260 - val_loss: 0.2052 - val_auc_2: 0.8873 - val_auc_3: 0.6028 - val_precision_1: 0.7341 - val_recall_1: 0.3960 - 192s/epoch - 19ms/step\n",
      "Epoch 29/200\n",
      "10076/10076 - 191s - loss: 0.1872 - auc_2: 0.9047 - auc_3: 0.6404 - precision_1: 0.7542 - recall_1: 0.4254 - val_loss: 0.2051 - val_auc_2: 0.8880 - val_auc_3: 0.6033 - val_precision_1: 0.7375 - val_recall_1: 0.3859 - 191s/epoch - 19ms/step\n",
      "Epoch 30/200\n",
      "10076/10076 - 192s - loss: 0.1870 - auc_2: 0.9053 - auc_3: 0.6412 - precision_1: 0.7612 - recall_1: 0.4239 - val_loss: 0.2051 - val_auc_2: 0.8879 - val_auc_3: 0.6044 - val_precision_1: 0.7236 - val_recall_1: 0.4020 - 192s/epoch - 19ms/step\n",
      "Epoch 31/200\n",
      "10076/10076 - 192s - loss: 0.1867 - auc_2: 0.9054 - auc_3: 0.6426 - precision_1: 0.7553 - recall_1: 0.4281 - val_loss: 0.2053 - val_auc_2: 0.8882 - val_auc_3: 0.6037 - val_precision_1: 0.6952 - val_recall_1: 0.4101 - 192s/epoch - 19ms/step\n",
      "Epoch 32/200\n",
      "10076/10076 - 192s - loss: 0.1865 - auc_2: 0.9054 - auc_3: 0.6439 - precision_1: 0.7612 - recall_1: 0.4254 - val_loss: 0.2053 - val_auc_2: 0.8882 - val_auc_3: 0.6037 - val_precision_1: 0.7010 - val_recall_1: 0.4121 - 192s/epoch - 19ms/step\n",
      "Epoch 33/200\n",
      "10076/10076 - 191s - loss: 0.1863 - auc_2: 0.9055 - auc_3: 0.6445 - precision_1: 0.7558 - recall_1: 0.4260 - val_loss: 0.2050 - val_auc_2: 0.8879 - val_auc_3: 0.6042 - val_precision_1: 0.6976 - val_recall_1: 0.4101 - 191s/epoch - 19ms/step\n",
      "Epoch 34/200\n",
      "10076/10076 - 191s - loss: 0.1860 - auc_2: 0.9058 - auc_3: 0.6457 - precision_1: 0.7550 - recall_1: 0.4322 - val_loss: 0.2043 - val_auc_2: 0.8883 - val_auc_3: 0.6079 - val_precision_1: 0.7174 - val_recall_1: 0.4000 - 191s/epoch - 19ms/step\n",
      "Epoch 35/200\n",
      "10076/10076 - 192s - loss: 0.1858 - auc_2: 0.9063 - auc_3: 0.6460 - precision_1: 0.7564 - recall_1: 0.4291 - val_loss: 0.2042 - val_auc_2: 0.8882 - val_auc_3: 0.6055 - val_precision_1: 0.7138 - val_recall_1: 0.4081 - 192s/epoch - 19ms/step\n",
      "Epoch 36/200\n",
      "10076/10076 - 191s - loss: 0.1855 - auc_2: 0.9061 - auc_3: 0.6472 - precision_1: 0.7541 - recall_1: 0.4364 - val_loss: 0.2042 - val_auc_2: 0.8886 - val_auc_3: 0.6074 - val_precision_1: 0.7259 - val_recall_1: 0.3960 - 191s/epoch - 19ms/step\n",
      "Epoch 37/200\n",
      "10076/10076 - 191s - loss: 0.1852 - auc_2: 0.9066 - auc_3: 0.6485 - precision_1: 0.7667 - recall_1: 0.4301 - val_loss: 0.2040 - val_auc_2: 0.8889 - val_auc_3: 0.6083 - val_precision_1: 0.6949 - val_recall_1: 0.4141 - 191s/epoch - 19ms/step\n",
      "Epoch 38/200\n",
      "10076/10076 - 192s - loss: 0.1851 - auc_2: 0.9065 - auc_3: 0.6487 - precision_1: 0.7548 - recall_1: 0.4333 - val_loss: 0.2041 - val_auc_2: 0.8883 - val_auc_3: 0.6084 - val_precision_1: 0.7034 - val_recall_1: 0.4121 - 192s/epoch - 19ms/step\n",
      "Epoch 39/200\n",
      "10076/10076 - 191s - loss: 0.1849 - auc_2: 0.9070 - auc_3: 0.6494 - precision_1: 0.7629 - recall_1: 0.4312 - val_loss: 0.2044 - val_auc_2: 0.8891 - val_auc_3: 0.6089 - val_precision_1: 0.6900 - val_recall_1: 0.4182 - 191s/epoch - 19ms/step\n",
      "Epoch 40/200\n",
      "10076/10076 - 192s - loss: 0.1847 - auc_2: 0.9070 - auc_3: 0.6503 - precision_1: 0.7579 - recall_1: 0.4374 - val_loss: 0.2035 - val_auc_2: 0.8889 - val_auc_3: 0.6105 - val_precision_1: 0.7088 - val_recall_1: 0.4081 - 192s/epoch - 19ms/step\n",
      "Epoch 41/200\n",
      "10076/10076 - 192s - loss: 0.1844 - auc_2: 0.9076 - auc_3: 0.6512 - precision_1: 0.7551 - recall_1: 0.4406 - val_loss: 0.2035 - val_auc_2: 0.8878 - val_auc_3: 0.6098 - val_precision_1: 0.7246 - val_recall_1: 0.4040 - 192s/epoch - 19ms/step\n",
      "Epoch 42/200\n",
      "10076/10076 - 192s - loss: 0.1843 - auc_2: 0.9075 - auc_3: 0.6518 - precision_1: 0.7557 - recall_1: 0.4353 - val_loss: 0.2033 - val_auc_2: 0.8899 - val_auc_3: 0.6099 - val_precision_1: 0.7007 - val_recall_1: 0.4162 - 192s/epoch - 19ms/step\n",
      "Epoch 43/200\n",
      "10076/10076 - 192s - loss: 0.1840 - auc_2: 0.9080 - auc_3: 0.6522 - precision_1: 0.7592 - recall_1: 0.4406 - val_loss: 0.2036 - val_auc_2: 0.8895 - val_auc_3: 0.6102 - val_precision_1: 0.6954 - val_recall_1: 0.4242 - 192s/epoch - 19ms/step\n",
      "Epoch 44/200\n",
      "10076/10076 - 192s - loss: 0.1835 - auc_2: 0.9077 - auc_3: 0.6542 - precision_1: 0.7633 - recall_1: 0.4505 - val_loss: 0.2034 - val_auc_2: 0.8894 - val_auc_3: 0.6111 - val_precision_1: 0.7432 - val_recall_1: 0.3859 - 192s/epoch - 19ms/step\n",
      "Epoch 45/200\n",
      "10076/10076 - 192s - loss: 0.1835 - auc_2: 0.9079 - auc_3: 0.6549 - precision_1: 0.7652 - recall_1: 0.4385 - val_loss: 0.2035 - val_auc_2: 0.8893 - val_auc_3: 0.6108 - val_precision_1: 0.6974 - val_recall_1: 0.4283 - 192s/epoch - 19ms/step\n",
      "Epoch 46/200\n",
      "10076/10076 - 192s - loss: 0.1833 - auc_2: 0.9082 - auc_3: 0.6549 - precision_1: 0.7567 - recall_1: 0.4395 - val_loss: 0.2027 - val_auc_2: 0.8895 - val_auc_3: 0.6125 - val_precision_1: 0.7085 - val_recall_1: 0.4222 - 192s/epoch - 19ms/step\n",
      "Epoch 47/200\n",
      "10076/10076 - 192s - loss: 0.1832 - auc_2: 0.9085 - auc_3: 0.6558 - precision_1: 0.7567 - recall_1: 0.4426 - val_loss: 0.2032 - val_auc_2: 0.8903 - val_auc_3: 0.6120 - val_precision_1: 0.6980 - val_recall_1: 0.4202 - 192s/epoch - 19ms/step\n",
      "Epoch 48/200\n",
      "10076/10076 - 192s - loss: 0.1830 - auc_2: 0.9087 - auc_3: 0.6558 - precision_1: 0.7607 - recall_1: 0.4442 - val_loss: 0.2028 - val_auc_2: 0.8905 - val_auc_3: 0.6129 - val_precision_1: 0.7047 - val_recall_1: 0.4242 - 192s/epoch - 19ms/step\n",
      "Epoch 49/200\n",
      "10076/10076 - 201s - loss: 0.1827 - auc_2: 0.9091 - auc_3: 0.6580 - precision_1: 0.7623 - recall_1: 0.4432 - val_loss: 0.2029 - val_auc_2: 0.8904 - val_auc_3: 0.6133 - val_precision_1: 0.7010 - val_recall_1: 0.4121 - 201s/epoch - 20ms/step\n",
      "Epoch 50/200\n",
      "10076/10076 - 197s - loss: 0.1826 - auc_2: 0.9091 - auc_3: 0.6575 - precision_1: 0.7584 - recall_1: 0.4468 - val_loss: 0.2027 - val_auc_2: 0.8901 - val_auc_3: 0.6147 - val_precision_1: 0.7059 - val_recall_1: 0.4121 - 197s/epoch - 20ms/step\n",
      "Epoch 51/200\n",
      "10076/10076 - 192s - loss: 0.1824 - auc_2: 0.9090 - auc_3: 0.6584 - precision_1: 0.7672 - recall_1: 0.4416 - val_loss: 0.2028 - val_auc_2: 0.8905 - val_auc_3: 0.6134 - val_precision_1: 0.7053 - val_recall_1: 0.4303 - 192s/epoch - 19ms/step\n",
      "Epoch 52/200\n",
      "10076/10076 - 192s - loss: 0.1822 - auc_2: 0.9094 - auc_3: 0.6601 - precision_1: 0.7673 - recall_1: 0.4453 - val_loss: 0.2029 - val_auc_2: 0.8903 - val_auc_3: 0.6127 - val_precision_1: 0.6941 - val_recall_1: 0.4263 - 192s/epoch - 19ms/step\n",
      "Epoch 53/200\n",
      "10076/10076 - 191s - loss: 0.1820 - auc_2: 0.9094 - auc_3: 0.6599 - precision_1: 0.7637 - recall_1: 0.4447 - val_loss: 0.2034 - val_auc_2: 0.8901 - val_auc_3: 0.6136 - val_precision_1: 0.6950 - val_recall_1: 0.4465 - 191s/epoch - 19ms/step\n",
      "Epoch 54/200\n",
      "10076/10076 - 192s - loss: 0.1817 - auc_2: 0.9093 - auc_3: 0.6609 - precision_1: 0.7632 - recall_1: 0.4520 - val_loss: 0.2029 - val_auc_2: 0.8881 - val_auc_3: 0.6118 - val_precision_1: 0.7425 - val_recall_1: 0.4020 - 192s/epoch - 19ms/step\n",
      "Epoch 55/200\n",
      "10076/10076 - 192s - loss: 0.1817 - auc_2: 0.9096 - auc_3: 0.6605 - precision_1: 0.7676 - recall_1: 0.4442 - val_loss: 0.2021 - val_auc_2: 0.8909 - val_auc_3: 0.6152 - val_precision_1: 0.7133 - val_recall_1: 0.4222 - 192s/epoch - 19ms/step\n",
      "Epoch 56/200\n",
      "10076/10076 - 192s - loss: 0.1815 - auc_2: 0.9102 - auc_3: 0.6620 - precision_1: 0.7710 - recall_1: 0.4494 - val_loss: 0.2020 - val_auc_2: 0.8908 - val_auc_3: 0.6159 - val_precision_1: 0.7177 - val_recall_1: 0.4263 - 192s/epoch - 19ms/step\n",
      "Epoch 57/200\n",
      "10076/10076 - 192s - loss: 0.1812 - auc_2: 0.9101 - auc_3: 0.6635 - precision_1: 0.7679 - recall_1: 0.4520 - val_loss: 0.2020 - val_auc_2: 0.8906 - val_auc_3: 0.6164 - val_precision_1: 0.7158 - val_recall_1: 0.4222 - 192s/epoch - 19ms/step\n",
      "Epoch 58/200\n",
      "10076/10076 - 191s - loss: 0.1812 - auc_2: 0.9100 - auc_3: 0.6634 - precision_1: 0.7670 - recall_1: 0.4479 - val_loss: 0.2024 - val_auc_2: 0.8906 - val_auc_3: 0.6157 - val_precision_1: 0.7072 - val_recall_1: 0.4343 - 191s/epoch - 19ms/step\n",
      "Epoch 59/200\n",
      "10076/10076 - 191s - loss: 0.1809 - auc_2: 0.9105 - auc_3: 0.6645 - precision_1: 0.7659 - recall_1: 0.4520 - val_loss: 0.2020 - val_auc_2: 0.8894 - val_auc_3: 0.6155 - val_precision_1: 0.7316 - val_recall_1: 0.4020 - 191s/epoch - 19ms/step\n",
      "Epoch 60/200\n",
      "10076/10076 - 192s - loss: 0.1808 - auc_2: 0.9104 - auc_3: 0.6647 - precision_1: 0.7709 - recall_1: 0.4510 - val_loss: 0.2020 - val_auc_2: 0.8912 - val_auc_3: 0.6158 - val_precision_1: 0.7222 - val_recall_1: 0.4202 - 192s/epoch - 19ms/step\n",
      "Epoch 61/200\n",
      "10076/10076 - 193s - loss: 0.1806 - auc_2: 0.9108 - auc_3: 0.6657 - precision_1: 0.7697 - recall_1: 0.4531 - val_loss: 0.2018 - val_auc_2: 0.8911 - val_auc_3: 0.6162 - val_precision_1: 0.7196 - val_recall_1: 0.4303 - 193s/epoch - 19ms/step\n",
      "Epoch 62/200\n",
      "10076/10076 - 191s - loss: 0.1803 - auc_2: 0.9114 - auc_3: 0.6662 - precision_1: 0.7730 - recall_1: 0.4546 - val_loss: 0.2018 - val_auc_2: 0.8911 - val_auc_3: 0.6178 - val_precision_1: 0.7097 - val_recall_1: 0.4444 - 191s/epoch - 19ms/step\n",
      "Epoch 63/200\n",
      "10076/10076 - 192s - loss: 0.1801 - auc_2: 0.9115 - auc_3: 0.6658 - precision_1: 0.7689 - recall_1: 0.4562 - val_loss: 0.2018 - val_auc_2: 0.8910 - val_auc_3: 0.6180 - val_precision_1: 0.7162 - val_recall_1: 0.4384 - 192s/epoch - 19ms/step\n",
      "Epoch 64/200\n",
      "10076/10076 - 192s - loss: 0.1800 - auc_2: 0.9114 - auc_3: 0.6669 - precision_1: 0.7714 - recall_1: 0.4520 - val_loss: 0.2018 - val_auc_2: 0.8918 - val_auc_3: 0.6182 - val_precision_1: 0.7045 - val_recall_1: 0.4384 - 192s/epoch - 19ms/step\n",
      "Epoch 65/200\n",
      "10076/10076 - 192s - loss: 0.1796 - auc_2: 0.9116 - auc_3: 0.6686 - precision_1: 0.7675 - recall_1: 0.4578 - val_loss: 0.2015 - val_auc_2: 0.8911 - val_auc_3: 0.6183 - val_precision_1: 0.7176 - val_recall_1: 0.4364 - 192s/epoch - 19ms/step\n",
      "Epoch 66/200\n",
      "10076/10076 - 191s - loss: 0.1797 - auc_2: 0.9118 - auc_3: 0.6690 - precision_1: 0.7700 - recall_1: 0.4557 - val_loss: 0.2020 - val_auc_2: 0.8914 - val_auc_3: 0.6177 - val_precision_1: 0.7068 - val_recall_1: 0.4384 - 191s/epoch - 19ms/step\n",
      "Epoch 67/200\n",
      "10076/10076 - 192s - loss: 0.1795 - auc_2: 0.9118 - auc_3: 0.6698 - precision_1: 0.7659 - recall_1: 0.4588 - val_loss: 0.2011 - val_auc_2: 0.8911 - val_auc_3: 0.6188 - val_precision_1: 0.7257 - val_recall_1: 0.4222 - 192s/epoch - 19ms/step\n",
      "Epoch 68/200\n",
      "10076/10076 - 193s - loss: 0.1793 - auc_2: 0.9121 - auc_3: 0.6695 - precision_1: 0.7679 - recall_1: 0.4536 - val_loss: 0.2008 - val_auc_2: 0.8913 - val_auc_3: 0.6203 - val_precision_1: 0.7304 - val_recall_1: 0.4323 - 193s/epoch - 19ms/step\n",
      "Epoch 69/200\n",
      "10076/10076 - 192s - loss: 0.1791 - auc_2: 0.9122 - auc_3: 0.6700 - precision_1: 0.7735 - recall_1: 0.4593 - val_loss: 0.2011 - val_auc_2: 0.8911 - val_auc_3: 0.6198 - val_precision_1: 0.7251 - val_recall_1: 0.4263 - 192s/epoch - 19ms/step\n",
      "Epoch 70/200\n",
      "10076/10076 - 192s - loss: 0.1790 - auc_2: 0.9121 - auc_3: 0.6708 - precision_1: 0.7745 - recall_1: 0.4567 - val_loss: 0.2016 - val_auc_2: 0.8918 - val_auc_3: 0.6183 - val_precision_1: 0.7070 - val_recall_1: 0.4485 - 192s/epoch - 19ms/step\n",
      "Epoch 71/200\n",
      "10076/10076 - 191s - loss: 0.1786 - auc_2: 0.9125 - auc_3: 0.6726 - precision_1: 0.7728 - recall_1: 0.4593 - val_loss: 0.2007 - val_auc_2: 0.8912 - val_auc_3: 0.6206 - val_precision_1: 0.7361 - val_recall_1: 0.4283 - 191s/epoch - 19ms/step\n",
      "Epoch 72/200\n",
      "10076/10076 - 191s - loss: 0.1785 - auc_2: 0.9126 - auc_3: 0.6722 - precision_1: 0.7776 - recall_1: 0.4557 - val_loss: 0.2015 - val_auc_2: 0.8924 - val_auc_3: 0.6196 - val_precision_1: 0.7157 - val_recall_1: 0.4424 - 191s/epoch - 19ms/step\n",
      "Epoch 73/200\n",
      "10076/10076 - 192s - loss: 0.1784 - auc_2: 0.9130 - auc_3: 0.6726 - precision_1: 0.7700 - recall_1: 0.4609 - val_loss: 0.2014 - val_auc_2: 0.8909 - val_auc_3: 0.6188 - val_precision_1: 0.7162 - val_recall_1: 0.4283 - 192s/epoch - 19ms/step\n",
      "Epoch 74/200\n",
      "10076/10076 - 191s - loss: 0.1782 - auc_2: 0.9130 - auc_3: 0.6737 - precision_1: 0.7734 - recall_1: 0.4572 - val_loss: 0.2012 - val_auc_2: 0.8914 - val_auc_3: 0.6208 - val_precision_1: 0.7134 - val_recall_1: 0.4525 - 191s/epoch - 19ms/step\n",
      "Epoch 75/200\n",
      "10076/10076 - 191s - loss: 0.1781 - auc_2: 0.9129 - auc_3: 0.6745 - precision_1: 0.7786 - recall_1: 0.4619 - val_loss: 0.2016 - val_auc_2: 0.8926 - val_auc_3: 0.6208 - val_precision_1: 0.7134 - val_recall_1: 0.4424 - 191s/epoch - 19ms/step\n",
      "Epoch 76/200\n",
      "10076/10076 - 191s - loss: 0.1779 - auc_2: 0.9135 - auc_3: 0.6752 - precision_1: 0.7801 - recall_1: 0.4625 - val_loss: 0.2014 - val_auc_2: 0.8916 - val_auc_3: 0.6214 - val_precision_1: 0.7138 - val_recall_1: 0.4485 - 191s/epoch - 19ms/step\n",
      "Epoch 77/200\n",
      "10076/10076 - 192s - loss: 0.1778 - auc_2: 0.9134 - auc_3: 0.6751 - precision_1: 0.7762 - recall_1: 0.4630 - val_loss: 0.2010 - val_auc_2: 0.8911 - val_auc_3: 0.6206 - val_precision_1: 0.7196 - val_recall_1: 0.4303 - 192s/epoch - 19ms/step\n",
      "Epoch 78/200\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "10076/10076 - 192s - loss: 0.1776 - auc_2: 0.9136 - auc_3: 0.6757 - precision_1: 0.7757 - recall_1: 0.4599 - val_loss: 0.2013 - val_auc_2: 0.8919 - val_auc_3: 0.6214 - val_precision_1: 0.7152 - val_recall_1: 0.4465 - 192s/epoch - 19ms/step\n",
      "Epoch 78: early stopping\n",
      "3149/3149 [==============================] - 35s 11ms/step - loss: 0.1983 - auc_2: 0.8972 - auc_3: 0.6051 - precision_1: 0.6950 - recall_1: 0.3917\n"
     ]
    }
   ],
   "source": [
    "cnn_mort_model, cnn_mort_preds  = train_eval_pred_model(class_model, 2, 200, mortality_cnn_X_train, mortality_cnn_X_test,\n",
    "                                       mortality_cnn_y_train, mortality_cnn_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_art6AQSVBY",
    "outputId": "67bf8cc2-501b-44dc-f6fb-2d1588926b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20129/20129 - 299s - loss: 0.8661 - mean_absolute_error: 0.5534 - val_loss: 1.1039 - val_mean_absolute_error: 0.5823 - 299s/epoch - 15ms/step\n",
      "Epoch 2/200\n",
      "20129/20129 - 299s - loss: 0.8468 - mean_absolute_error: 0.5508 - val_loss: 1.1113 - val_mean_absolute_error: 0.5815 - 299s/epoch - 15ms/step\n",
      "Epoch 3/200\n",
      "20129/20129 - 295s - loss: 0.8352 - mean_absolute_error: 0.5468 - val_loss: 1.1581 - val_mean_absolute_error: 0.6039 - 295s/epoch - 15ms/step\n",
      "Epoch 4/200\n",
      "20129/20129 - 298s - loss: 0.8222 - mean_absolute_error: 0.5434 - val_loss: 1.1255 - val_mean_absolute_error: 0.5818 - 298s/epoch - 15ms/step\n",
      "Epoch 5/200\n",
      "20129/20129 - 296s - loss: 0.7990 - mean_absolute_error: 0.5401 - val_loss: 1.1327 - val_mean_absolute_error: 0.5845 - 296s/epoch - 15ms/step\n",
      "Epoch 6/200\n",
      "20129/20129 - 295s - loss: 0.7864 - mean_absolute_error: 0.5345 - val_loss: 1.1532 - val_mean_absolute_error: 0.5910 - 295s/epoch - 15ms/step\n",
      "Epoch 7/200\n",
      "20129/20129 - 296s - loss: 0.7764 - mean_absolute_error: 0.5305 - val_loss: 1.1533 - val_mean_absolute_error: 0.5893 - 296s/epoch - 15ms/step\n",
      "Epoch 8/200\n",
      "20129/20129 - 297s - loss: 0.7411 - mean_absolute_error: 0.5273 - val_loss: 1.2022 - val_mean_absolute_error: 0.6102 - 297s/epoch - 15ms/step\n",
      "Epoch 9/200\n",
      "20129/20129 - 301s - loss: 0.7280 - mean_absolute_error: 0.5236 - val_loss: 1.1476 - val_mean_absolute_error: 0.5880 - 301s/epoch - 15ms/step\n",
      "Epoch 10/200\n",
      "20129/20129 - 298s - loss: 0.7056 - mean_absolute_error: 0.5198 - val_loss: 1.1747 - val_mean_absolute_error: 0.5931 - 298s/epoch - 15ms/step\n",
      "Epoch 11/200\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "20129/20129 - 291s - loss: 0.6823 - mean_absolute_error: 0.5145 - val_loss: 1.1533 - val_mean_absolute_error: 0.5885 - 291s/epoch - 14ms/step\n",
      "Epoch 11: early stopping\n",
      "6285/6285 [==============================] - 44s 7ms/step - loss: 0.9005 - mean_absolute_error: 0.6463\n"
     ]
    }
   ],
   "source": [
    "cnn_los_model, cnn_los_preds = train_eval_pred_model(reg_model, 1, 200, los_cnn_X_train, los_cnn_X_test, los_cnn_y_train, los_cnn_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8rOI-Y2-p93",
    "outputId": "97b49260-d150-4131-dc85-a9710e7e707a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "los_cnn_y_test_std = np.subtract(los_cnn_y_test, np.repeat(np.mean(los_cnn_y_test), len(los_cnn_y_test))) / np.std(los_cnn_y_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(los_cnn_y_test_std, cnn_los_preds, s=25, zorder=10)\n",
    "\n",
    "\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37P85e7t-p93"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics\n",
    "\n",
    "# Implementing Logistic Regression Model\n",
    "def train_test_lr_model(X_train, X_test, y_train, y_test): \n",
    "    \n",
    "    lr_model = LogisticRegression(random_state=random_seed, max_iter=10e6)\n",
    "    \n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = lr_model.predict_proba(X_test)\n",
    "    \n",
    "    auroc = metrics.roc_auc_score(y_test, preds[:,1])\n",
    "    print(\"auroc: \", auroc, \"\\n\")\n",
    "    \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, preds[:,1])\n",
    "    \n",
    "    auprc = metrics.auc(recall, precision)\n",
    "    print(\"auprc: \", auprc, \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw8ufJVM-p93"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "\n",
    "# Implementing Linear Regression Model\n",
    "def train_test_linear_model(X_train, X_test, y_train, y_test): \n",
    "    \n",
    "    lr_model = Lasso(random_state=random_seed, max_iter=10e6)\n",
    "    \n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = lr_model.predict(X_test)\n",
    "    \n",
    "    mse = metrics.mean_squared_error(y_test, preds)\n",
    "    print(\"mse: \", mse, \"\\n\")\n",
    "    \n",
    "    mae = metrics.mean_absolute_error(y_test, preds)\n",
    "\n",
    "    print(\"mae: \", mae, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CuNREcR-p93",
    "outputId": "0568a615-cfa3-4c54-f4a3-9b87af2db97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.561354868089402 \n",
      "\n",
      "auprc:  0.16360493245542496 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR mean readmission\n",
    "\n",
    "readm_mean_X_train_2d = readm_mean_X_train.reshape((readm_mean_X_train.shape[0], readm_mean_X_train.shape[1]*readm_mean_X_train.shape[2]))\n",
    "\n",
    "readm_mean_X_test_2d = readm_mean_X_test.reshape((readm_mean_X_test.shape[0], readm_mean_X_test.shape[1]*readm_mean_X_test.shape[2]))\n",
    "\n",
    "train_test_lr_model(readm_mean_X_train_2d, readm_mean_X_test_2d, readm_mean_y_train, readm_mean_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNktSqEY-p93",
    "outputId": "151a2264-31f3-4bac-a194-3af4c79442e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.7747496200205269 \n",
      "\n",
      "auprc:  0.3792769789916107 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR mean ihm\n",
    "\n",
    "\n",
    "mortality_mean_X_train_2d = mortality_mean_X_train.reshape((mortality_mean_X_train.shape[0],\n",
    "                                                            mortality_mean_X_train.shape[1]*mortality_mean_X_train.shape[2]))\n",
    "\n",
    "mortality_mean_X_test_2d = mortality_mean_X_test.reshape((mortality_mean_X_test.shape[0],\n",
    "                                                       mortality_mean_X_test.shape[1]*mortality_mean_X_test.shape[2]))\n",
    "\n",
    "train_test_lr_model(mortality_mean_X_train_2d, mortality_mean_X_test_2d, mortality_mean_y_train, mortality_mean_y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmsUxjkG-p93",
    "outputId": "c100c27c-91d4-461e-d7c8-f107c24df3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  8.787901536306133e+16 \n",
      "\n",
      "mae:  3750157.5595066603 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear regression mean los\n",
    "\n",
    "los_mean_X_train_2d = los_mean_X_train.reshape((los_mean_X_train.shape[0], los_mean_X_train.shape[1]*los_mean_X_train.shape[2]))\n",
    "\n",
    "los_mean_X_test_2d = los_mean_X_test.reshape((los_mean_X_test.shape[0], los_mean_X_test.shape[1]*los_mean_X_test.shape[2]))\n",
    "\n",
    "train_test_linear_model(los_mean_X_train_2d, los_mean_X_test_2d, los_mean_y_train, los_mean_y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fhHa7_E-p94",
    "outputId": "cb3f1311-317d-47ef-954d-6079054b5c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.560741450769777 \n",
      "\n",
      "auprc:  0.16232186816400582 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR cnn readmission\n",
    "\n",
    "\n",
    "readm_cnn_X_train_2d = readm_cnn_X_train.reshape((readm_cnn_X_train.shape[0], readm_cnn_X_train.shape[1]*readm_cnn_X_train.shape[2]))\n",
    "\n",
    "readm_cnn_X_test_2d = readm_cnn_X_test.reshape((readm_cnn_X_test.shape[0], readm_cnn_X_test.shape[1]*readm_cnn_X_test.shape[2]))\n",
    "\n",
    "train_test_lr_model(readm_cnn_X_train_2d, readm_cnn_X_test_2d, readm_cnn_y_train, readm_cnn_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16id9BOQ-p94",
    "outputId": "ddefa5d6-4a2e-493a-e39d-d053657d712e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.8121227605889791 \n",
      "\n",
      "auprc:  0.4078001630668505 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LR cnn ihm\n",
    "\n",
    "\n",
    "\n",
    "mortality_cnn_X_train_2d = mortality_cnn_X_train.reshape((mortality_cnn_X_train.shape[0],\n",
    "                                                          mortality_cnn_X_train.shape[1]*mortality_cnn_X_train.shape[2]))\n",
    "\n",
    "mortality_cnn_X_test_2d = mortality_cnn_X_test.reshape((mortality_cnn_X_test.shape[0],\n",
    "                                                     mortality_cnn_X_test.shape[1]*mortality_cnn_X_test.shape[2]))\n",
    "\n",
    "train_test_lr_model(mortality_cnn_X_train_2d, mortality_cnn_X_test_2d, mortality_cnn_y_train, mortality_cnn_y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3FmQQ0W-p94",
    "outputId": "12ab22f1-da13-4109-8e4d-f2e2d802502e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  1.0 \n",
      "\n",
      "mae:  0.6456541924088668 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear regression cnn los\n",
    "\n",
    "\n",
    "los_cnn_X_train_2d = los_cnn_X_train.reshape((los_cnn_X_train.shape[0], los_cnn_X_train.shape[1]*los_cnn_X_train.shape[2]))\n",
    "\n",
    "los_cnn_X_test_2d = los_cnn_X_test.reshape((los_cnn_X_test.shape[0], los_cnn_X_test.shape[1]*los_cnn_X_test.shape[2]))\n",
    "\n",
    "train_test_linear_model(los_cnn_X_train_2d, los_cnn_X_test_2d, los_cnn_y_train, los_cnn_y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ydz86Dku-p94"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Implementing XGBoost model\n",
    "def train_test_XGBoost_class(X_train, X_test, y_train, y_test):\n",
    "    xg_class = xgb.XGBClassifier(objective ='binary:logistic', nthread=1, learning_rate = 0.2,\n",
    "                    max_depth = 12, alpha = 12, n_estimators = 35, use_label_encoder = False, eval_metric='logloss')\n",
    "    \n",
    "    xg_class.fit(X_train,y_train)\n",
    "\n",
    "    preds = xg_class.predict_proba(X_test)\n",
    "    \n",
    "    auroc = metrics.roc_auc_score(y_test, preds[:,1])\n",
    "    print(\"auroc: \", auroc, \"\\n\")\n",
    "    \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, preds[:,1])\n",
    "    \n",
    "    auprc = metrics.auc(recall, precision)\n",
    "    print(\"auprc: \", auprc, \"\\n\")\n",
    "    \n",
    "\n",
    "     \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ67f3hw-p94"
   },
   "outputs": [],
   "source": [
    "def train_test_XGBoost_reg(X_train, X_test, y_train, y_test):\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', nthread=1, learning_rate = 0.2,\n",
    "                    max_depth = 12, alpha = 12, n_estimators = 30)\n",
    "    \n",
    "    xg_reg.fit(X_train,y_train)\n",
    "\n",
    "    preds = xg_reg.predict(X_test)\n",
    "    \n",
    "    mse = metrics.mean_squared_error(y_test, preds)\n",
    "    print(\"mse: \", mse, \"\\n\")\n",
    "    \n",
    "    mae = metrics.mean_absolute_error(y_test, preds)\n",
    "    print(\"mae: \", mae, \"\\n\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__DjDkSG-p95",
    "outputId": "19126f33-02be-4547-ddcd-b8c4b363b94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.5621806527938551 \n",
      "\n",
      "auprc:  0.17173605577090123 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB mean readmission\n",
    "\n",
    "readm_mean_X_train_2d = readm_mean_X_train.reshape((readm_mean_X_train.shape[0], readm_mean_X_train.shape[1]*readm_mean_X_train.shape[2]))\n",
    "\n",
    "readm_mean_X_test_2d = readm_mean_X_test.reshape((readm_mean_X_test.shape[0], readm_mean_X_test.shape[1]*readm_mean_X_test.shape[2]))\n",
    "\n",
    "train_test_XGBoost_class(readm_mean_X_train_2d, readm_mean_X_test_2d, readm_mean_y_train, readm_mean_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_Z0YJDX-p95",
    "outputId": "97beb20f-de29-4861-a378-416294d34716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.8863928878875464 \n",
      "\n",
      "auprc:  0.5721580850054485 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB mean ihm\n",
    "\n",
    "\n",
    "mortality_mean_X_train_2d = mortality_mean_X_train.reshape((mortality_mean_X_train.shape[0],\n",
    "                                                            mortality_mean_X_train.shape[1]*mortality_mean_X_train.shape[2]))\n",
    "\n",
    "mortality_mean_X_test_2d = mortality_mean_X_test.reshape((mortality_mean_X_test.shape[0],\n",
    "                                                       mortality_mean_X_test.shape[1]*mortality_mean_X_test.shape[2]))\n",
    "\n",
    "train_test_XGBoost_class(mortality_mean_X_train_2d, mortality_mean_X_test_2d, mortality_mean_y_train, mortality_mean_y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2fXZl1g-p95",
    "outputId": "0ddc296e-1745-4108-94b7-dc6bc8d915d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  0.8817036930970076 \n",
      "\n",
      "mae:  0.5756855323663213 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB mean los\n",
    "\n",
    "los_mean_X_train_2d = los_mean_X_train.reshape((los_mean_X_train.shape[0], los_mean_X_train.shape[1]*los_mean_X_train.shape[2]))\n",
    "\n",
    "los_mean_X_test_2d = los_mean_X_test.reshape((los_mean_X_test.shape[0], los_mean_X_test.shape[1]*los_mean_X_test.shape[2]))\n",
    "\n",
    "train_test_XGBoost_reg(los_mean_X_train_2d, los_mean_X_test_2d, los_mean_y_train, los_mean_y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2FAbIkb-p95",
    "outputId": "58b65bb4-06d1-4433-8a8f-bee6af812c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.547041266545549 \n",
      "\n",
      "auprc:  0.1637594250452039 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB cnn readmission\n",
    "\n",
    "\n",
    "readm_cnn_X_train_2d = readm_cnn_X_train.reshape((readm_cnn_X_train.shape[0], readm_cnn_X_train.shape[1]*readm_cnn_X_train.shape[2]))\n",
    "\n",
    "readm_cnn_X_test_2d = readm_cnn_X_test.reshape((readm_cnn_X_test.shape[0], readm_cnn_X_test.shape[1]*readm_cnn_X_test.shape[2]))\n",
    "\n",
    "train_test_XGBoost_class(readm_cnn_X_train_2d, readm_cnn_X_test_2d, readm_cnn_y_train, readm_cnn_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOsPXshV-p95",
    "outputId": "f0df361c-ccf7-45fe-dc9a-7d2e379b6d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc:  0.8877444099103873 \n",
      "\n",
      "auprc:  0.5738970708566882 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB cnn ihm\n",
    "\n",
    "\n",
    "\n",
    "mortality_cnn_X_train_2d = mortality_cnn_X_train.reshape((mortality_cnn_X_train.shape[0],\n",
    "                                                          mortality_cnn_X_train.shape[1]*mortality_cnn_X_train.shape[2]))\n",
    "\n",
    "mortality_cnn_X_test_2d = mortality_cnn_X_test.reshape((mortality_cnn_X_test.shape[0],\n",
    "                                                     mortality_cnn_X_test.shape[1]*mortality_cnn_X_test.shape[2]))\n",
    "\n",
    "train_test_XGBoost_class(mortality_cnn_X_train_2d, mortality_cnn_X_test_2d, mortality_cnn_y_train, mortality_cnn_y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve4Lru8j-p95",
    "outputId": "5e986c46-a30c-4fcd-bd13-dfa45496d066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  0.8619276186468752 \n",
      "\n",
      "mae:  0.5699887764981945 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGB cnn los\n",
    "\n",
    "\n",
    "los_cnn_X_train_2d = los_cnn_X_train.reshape((los_cnn_X_train.shape[0], los_cnn_X_train.shape[1]*los_cnn_X_train.shape[2]))\n",
    "\n",
    "los_cnn_X_test_2d = los_cnn_X_test.reshape((los_cnn_X_test.shape[0], los_cnn_X_test.shape[1]*los_cnn_X_test.shape[2]))\n",
    "\n",
    "train_test_XGBoost_reg(los_cnn_X_train_2d, los_cnn_X_test_2d, los_cnn_y_train, los_cnn_y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Snd4LKj--p96",
    "outputId": "f6b74b55-c1fe-48ca-c81d-c47feced4f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best auroc: 0.6507959216063358 \n",
      "\n",
      "depth: 12 \n",
      "\n",
      "n_est: 35 \n",
      "\n",
      "learning rate:  0.2 \n",
      "\n",
      "alpha:  12 \n",
      "\n",
      "Best auprc: 0.5717182456854938 \n",
      "\n",
      "depth: 12 \n",
      "\n",
      "n_est: 35 \n",
      "\n",
      "learning rate:  0.2 \n",
      "\n",
      "alpha:  12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross-validation xgb regression\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# Grid search for XGB\n",
    "depths = [12, 15]\n",
    "n_ests = [30, 35]\n",
    "lrs = [0.1, 0.2]\n",
    "alphas = [12, 14]\n",
    "\n",
    "\n",
    "auroc_dict = {}\n",
    "auprc_dict = {}\n",
    "\n",
    "for depth in depths:\n",
    "    for n_est in n_ests:\n",
    "        for lr in lrs:\n",
    "            for alpha in alphas:\n",
    "\n",
    "                auroc_dict[(depth, n_est, lr, alpha)] = 0\n",
    "                auprc_dict[(depth, n_est, lr, alpha)] = 0\n",
    "\n",
    "k = 4\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "for train_index, test_index in skf.split(mortality_mean_X_train_2d, mortality_mean_y_train):\n",
    "    X_train_kf, X_test_kf = mortality_mean_X_train_2d[train_index], mortality_mean_X_train_2d[test_index]\n",
    "    y_train_kf, y_test_kf = mortality_mean_y_train.iloc[train_index], mortality_mean_y_train.iloc[test_index]\n",
    "  \n",
    "\n",
    "    for depth in depths:\n",
    "        for n_est in n_ests:\n",
    "            for lr in lrs:\n",
    "                for alpha in alphas:\n",
    "                    \n",
    "                    \n",
    "                    xg_class = xgb.XGBClassifier(objective ='binary:logistic', nthread=1, learning_rate = lr,\n",
    "                    max_depth = depth, alpha = alpha, n_estimators = n_est, use_label_encoder = False, eval_metric='logloss')\n",
    "    \n",
    "                    xg_class.fit(X_train_kf, y_train_kf)\n",
    "\n",
    "                    preds = xg_class.predict_proba(X_test_kf)\n",
    "\n",
    "                    cur_auroc = metrics.roc_auc_score(y_test_kf, preds[:,1])\n",
    "\n",
    "                    precision, recall, thresholds = metrics.precision_recall_curve(y_test_kf, preds[:,1])\n",
    "\n",
    "                    cur_auprc = metrics.auc(recall, precision)\n",
    "\n",
    "                    auroc_dict[(depth, n_est, lr, alpha)] += cur_auroc / k \n",
    "                    auprc_dict[(depth, n_est, lr, alpha)] += cur_auprc / k\n",
    "\n",
    "\n",
    "best_auroc = -1\n",
    "best_auprc = -1\n",
    "\n",
    "for key, model_auroc in auroc_dict.items():\n",
    "    if model_auroc > best_auroc:\n",
    "        best_auroc = model_auroc\n",
    "        best_depth = key[0]\n",
    "        best_n_est = key[1]\n",
    "        best_learning_rate = key[2]\n",
    "        best_alpha = key[3]\n",
    "\n",
    "\n",
    "# Finding the best parameters n for mse\n",
    "print('Best auroc:', best_auroc, '\\n')\n",
    "print('depth:', best_depth, '\\n')\n",
    "print('n_est:', best_n_est, '\\n')\n",
    "print('learning rate: ', best_learning_rate, '\\n')\n",
    "print('alpha: ', best_alpha, '\\n')\n",
    "\n",
    "for key, model_auprc in auprc_dict.items():\n",
    "    if model_auprc > best_auprc:\n",
    "        best_auprc = model_auprc\n",
    "        best_depth = key[0]\n",
    "        best_n_est = key[1]\n",
    "        best_learning_rate = key[2]\n",
    "        best_alpha = key[3]\n",
    "  \n",
    " # Finding the best parameters n for mae\n",
    "print('Best auprc:', best_auprc, '\\n')\n",
    "print('depth:', best_depth, '\\n')\n",
    "print('n_est:', best_n_est, '\\n')\n",
    "print('learning rate: ', best_learning_rate, '\\n')\n",
    "print('alpha: ', best_alpha, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34MpHND8-p96"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of mimic_iv_testing_all.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
